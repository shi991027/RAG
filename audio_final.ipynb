{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZ/ueQO8ykEWzNxwK1Dfox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "636e6ef4e88d439c8c25a6952fb0405f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24d30918af364a4c81ef041ea2864219",
              "IPY_MODEL_edece0c8172347b2bc9610b45a642180",
              "IPY_MODEL_c664d8d516b14c9680ccce821e681b68"
            ],
            "layout": "IPY_MODEL_45c071ada7df48a8b0323b3b2a5cf92b"
          }
        },
        "24d30918af364a4c81ef041ea2864219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2337a472517a4300a9d34541ed141f46",
            "placeholder": "​",
            "style": "IPY_MODEL_21e4148b864b4f5180c94cc860018d61",
            "value": "modules.json: 100%"
          }
        },
        "edece0c8172347b2bc9610b45a642180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_239177015db74b489264a24670c0c9b4",
            "max": 141,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2e0a733fce04a179eb0ea877700fd75",
            "value": 141
          }
        },
        "c664d8d516b14c9680ccce821e681b68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02fe3e5894a3458a8f4f273047c18995",
            "placeholder": "​",
            "style": "IPY_MODEL_8e88f17253a747deaedb1ed138d2b46b",
            "value": " 141/141 [00:00&lt;00:00, 9.68kB/s]"
          }
        },
        "45c071ada7df48a8b0323b3b2a5cf92b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2337a472517a4300a9d34541ed141f46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21e4148b864b4f5180c94cc860018d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "239177015db74b489264a24670c0c9b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2e0a733fce04a179eb0ea877700fd75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02fe3e5894a3458a8f4f273047c18995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e88f17253a747deaedb1ed138d2b46b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "684869f67bfd41afa5787942f0dceeda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42d20c31729944e3a02ec433f87683f4",
              "IPY_MODEL_e7ef1e88ad8645459e657af02f97a701",
              "IPY_MODEL_fa9ad47960ad4888bba9c740c2cbcdbf"
            ],
            "layout": "IPY_MODEL_dd475438474f486a8ac32238f040d375"
          }
        },
        "42d20c31729944e3a02ec433f87683f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cbb825ae4764de69d4695d425097ab9",
            "placeholder": "​",
            "style": "IPY_MODEL_c0f9e6f7f5b345e2b16954f79ae3d5bf",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "e7ef1e88ad8645459e657af02f97a701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39260c9068644f62be5de51f43bb9682",
            "max": 214,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8653954388454ed58dc6f2c49a7d7c3e",
            "value": 214
          }
        },
        "fa9ad47960ad4888bba9c740c2cbcdbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a35f0400c7e54684a77309e4909bbc85",
            "placeholder": "​",
            "style": "IPY_MODEL_31566f2a0c5d4f5c8788f783d5b1a29b",
            "value": " 214/214 [00:00&lt;00:00, 19.0kB/s]"
          }
        },
        "dd475438474f486a8ac32238f040d375": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cbb825ae4764de69d4695d425097ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0f9e6f7f5b345e2b16954f79ae3d5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39260c9068644f62be5de51f43bb9682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8653954388454ed58dc6f2c49a7d7c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a35f0400c7e54684a77309e4909bbc85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31566f2a0c5d4f5c8788f783d5b1a29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc4bbfff125b47709fa7bc109cb8dd71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_319ef3e1e9984897b813f46a0b2e2b27",
              "IPY_MODEL_ba1235022bae4ebaa1e49e2fa4a33fd4",
              "IPY_MODEL_78132e0663474396aa2edb523953c4b8"
            ],
            "layout": "IPY_MODEL_2f53926e35954c77967f27f11bee3488"
          }
        },
        "319ef3e1e9984897b813f46a0b2e2b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_637812927d02424593e0920f7f5569e0",
            "placeholder": "​",
            "style": "IPY_MODEL_33fc3f3abdf842c0941ac1ed22d2bb4a",
            "value": "README.md: 100%"
          }
        },
        "ba1235022bae4ebaa1e49e2fa4a33fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10c3d717fadb49e5adea739b07392735",
            "max": 149181,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6a7c43f67d94b37a558610db59db431",
            "value": 149181
          }
        },
        "78132e0663474396aa2edb523953c4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8bc124f811b44caa689f4128ac8d797",
            "placeholder": "​",
            "style": "IPY_MODEL_3a501de355e64f1389ccd69bc985c55e",
            "value": " 149k/149k [00:00&lt;00:00, 1.98MB/s]"
          }
        },
        "2f53926e35954c77967f27f11bee3488": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "637812927d02424593e0920f7f5569e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33fc3f3abdf842c0941ac1ed22d2bb4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10c3d717fadb49e5adea739b07392735": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6a7c43f67d94b37a558610db59db431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8bc124f811b44caa689f4128ac8d797": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a501de355e64f1389ccd69bc985c55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecfd819ceadb4caeb8be247e025183f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2d64f7898f146299e4ac3c7f9ce9f7c",
              "IPY_MODEL_0c015bfac7174c31ab9689f3f916bcf9",
              "IPY_MODEL_27d0cfa2b2fe44e989736522ac6ef6ed"
            ],
            "layout": "IPY_MODEL_d552f313c7fb426d8c36c6ed256762b9"
          }
        },
        "d2d64f7898f146299e4ac3c7f9ce9f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e1c83d6e320421e8269aeaaa7f2c8c0",
            "placeholder": "​",
            "style": "IPY_MODEL_cf79c016a5af48fda6cd237a47e8bed1",
            "value": "model.safetensors: 100%"
          }
        },
        "0c015bfac7174c31ab9689f3f916bcf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2544d2ebc3e4f3e8e08fd07a3674036",
            "max": 433680480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f75c0ef032b4078bbe13681274c053d",
            "value": 433680480
          }
        },
        "27d0cfa2b2fe44e989736522ac6ef6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4f148252fae4f3ba7358f1048832e69",
            "placeholder": "​",
            "style": "IPY_MODEL_d61ea38399714409a23534041e87316b",
            "value": " 434M/434M [00:03&lt;00:00, 95.9MB/s]"
          }
        },
        "d552f313c7fb426d8c36c6ed256762b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e1c83d6e320421e8269aeaaa7f2c8c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf79c016a5af48fda6cd237a47e8bed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2544d2ebc3e4f3e8e08fd07a3674036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f75c0ef032b4078bbe13681274c053d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4f148252fae4f3ba7358f1048832e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d61ea38399714409a23534041e87316b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d417ef9f784f4015b99154fb92c23174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a72dc5ded79849cdadfadff9d1198311",
              "IPY_MODEL_053eb69f497644b5ae81f2e5e4ef3622",
              "IPY_MODEL_6364fc92098d459e8b08469ed157881b"
            ],
            "layout": "IPY_MODEL_857fd486a71f470980cbe9de8a8b6a5c"
          }
        },
        "a72dc5ded79849cdadfadff9d1198311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffd9623fa396495f9c30495be4adcc6a",
            "placeholder": "​",
            "style": "IPY_MODEL_13a66848a8df4651ba668974caf883ed",
            "value": "tokenizer.json: 100%"
          }
        },
        "053eb69f497644b5ae81f2e5e4ef3622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d21ea6ed69764920a50384b7e5753736",
            "max": 2563370,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8633b5e2b74e42c498d011c96a3fea51",
            "value": 2563370
          }
        },
        "6364fc92098d459e8b08469ed157881b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_990c10bc04974b57a0203d26afb10f48",
            "placeholder": "​",
            "style": "IPY_MODEL_d952073d457e40bda5dc008f2ff29d16",
            "value": " 2.56M/2.56M [00:01&lt;00:00, 1.50MB/s]"
          }
        },
        "857fd486a71f470980cbe9de8a8b6a5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffd9623fa396495f9c30495be4adcc6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13a66848a8df4651ba668974caf883ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d21ea6ed69764920a50384b7e5753736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8633b5e2b74e42c498d011c96a3fea51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "990c10bc04974b57a0203d26afb10f48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d952073d457e40bda5dc008f2ff29d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shi991027/RAG/blob/main/audio_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo3Q-KePOM1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93af998f-e066-4ddb-84a3-780a368cd515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m927.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting deepseek_tokenizer\n",
            "  Downloading deepseek_tokenizer-0.1.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tokenizers>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from deepseek_tokenizer) (0.21.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.20.0->deepseek_tokenizer) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.0->deepseek_tokenizer) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.0->deepseek_tokenizer) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.0->deepseek_tokenizer) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.0->deepseek_tokenizer) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.0->deepseek_tokenizer) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.0->deepseek_tokenizer) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.0->deepseek_tokenizer) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.0->deepseek_tokenizer) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.0->deepseek_tokenizer) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.0->deepseek_tokenizer) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.20.0->deepseek_tokenizer) (2025.1.31)\n",
            "Downloading deepseek_tokenizer-0.1.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deepseek_tokenizer\n",
            "Successfully installed deepseek_tokenizer-0.1.3\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.72.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Downloading sentence_transformers-4.0.2-py3-none-any.whl (340 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.6/340.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.4.1\n",
            "    Uninstalling sentence-transformers-3.4.1:\n",
            "      Successfully uninstalled sentence-transformers-3.4.1\n",
            "Successfully installed sentence-transformers-4.0.2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting json_repair\n",
            "  Downloading json_repair-0.41.1-py3-none-any.whl.metadata (11 kB)\n",
            "Downloading json_repair-0.41.1-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: json_repair\n",
            "Successfully installed json_repair-0.41.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "!pip install deepseek_tokenizer\n",
        "!pip install openai\n",
        "!pip install -U sentence-transformers\n",
        "!pip install -Uqq fastembed\n",
        "!pip install json_repair"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dashscope"
      ],
      "metadata": {
        "id": "ZeGCwUEKORdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8bf7c80-a6a2-40f2-e2fd-7589120c3075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dashscope\n",
            "  Downloading dashscope-1.23.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from dashscope) (3.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dashscope) (2.32.3)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from dashscope) (1.8.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->dashscope) (1.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dashscope) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dashscope) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dashscope) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dashscope) (2025.1.31)\n",
            "Downloading dashscope-1.23.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dashscope\n",
            "Successfully installed dashscope-1.23.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from openai.resources.chat import Completions\n",
        "from typing import Optional, Dict, Any, List\n",
        "from enum import Enum, auto\n",
        "import dashscope\n",
        "\n",
        "class APIVendor(Enum):\n",
        "    \"\"\"API厂商枚举类\"\"\"\n",
        "    ALIYUN = auto()\n",
        "    DEEPSEEK = auto()\n",
        "    ALIYUNAUDIO = auto()\n",
        "    ALIYUNVIDEO=auto()\n",
        "    @property\n",
        "    def config(self):\n",
        "        \"\"\"获取厂商配置\"\"\"\n",
        "        return {\n",
        "            APIVendor.ALIYUN: {\n",
        "                'api_key': 'sk-7f9a260343a54674b720a2d5fa772a5d',\n",
        "                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
        "                'suported_models': [\n",
        "                    'qwen-vl-plus',\n",
        "                ]\n",
        "            },\n",
        "            APIVendor.DEEPSEEK: {\n",
        "                'api_key': 'sk-c553c5fa02e64d729f91e8593f914776',\n",
        "                'base_url': 'https://api.deepseek.com',\n",
        "                'suported_models': [\n",
        "                    'deepseek-chat',\n",
        "                ]\n",
        "            },\n",
        "            APIVendor.ALIYUNAUDIO: {\n",
        "                'api_key': 'sk-7f9a260343a54674b720a2d5fa772a5d',\n",
        "                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
        "                'suported_models': [\n",
        "                    'qwen-audio-turbo-latest',\n",
        "                ]\n",
        "            },\n",
        "            APIVendor.ALIYUNVIDEO: {\n",
        "                'api_key': 'sk-7f9a260343a54674b720a2d5fa772a5d',\n",
        "                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
        "                'suported_models': [\n",
        "                    'qwen-vl-plus',\n",
        "                ]\n",
        "            }\n",
        "        }[self]\n",
        "\n",
        "# 自定义Completions类来重写create方法\n",
        "class CustomCompletions(Completions):\n",
        "    def __init__(self, client):\n",
        "        super().__init__(client)\n",
        "        self.client = client\n",
        "\n",
        "    def create(self, messages=None, **params):\n",
        "        \"\"\"重写create方法，根据vendor类型适配不同的API调用\"\"\"\n",
        "        if self.client.vendor == APIVendor.ALIYUNAUDIO:\n",
        "\n",
        "            model=params.get('model')\n",
        "            if not model:\n",
        "              model=self.client.vendor.config['suported_models'][0]\n",
        "            stream_value = params.get('stream', False)\n",
        "            return dashscope.MultiModalConversation.call(\n",
        "                api_key=self.client.vendor.config['api_key'],\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "                stream=stream_value,\n",
        "                incremental_output=stream_value\n",
        "            )\n",
        "        else:\n",
        "            # 调用原始的OpenAI方法\n",
        "            return super().create(messages=messages, **params)\n",
        "\n",
        "# 自定义Chat资源来使用我们的CustomCompletions\n",
        "class CustomChat:\n",
        "    def __init__(self, client):\n",
        "        self.completions = CustomCompletions(client)\n",
        "        self.client = client\n",
        "\n",
        "# LLM客户端扩展OpenAI但替换chat资源\n",
        "class LLMClient(OpenAI):\n",
        "    \"\"\"统一的LLM客户端，支持多种模型\"\"\"\n",
        "\n",
        "    def __init__(self, vendor: Optional[APIVendor] = APIVendor.DEEPSEEK, **kwargs):\n",
        "        \"\"\"\n",
        "        初始化LLM客户端\n",
        "\n",
        "        Args:\n",
        "            vendor: 指定厂商枚举，如果为None则使用默认厂商\n",
        "            **kwargs: 父类OpenAI支持的初始化参数\n",
        "        \"\"\"\n",
        "        if vendor:\n",
        "            assert vendor in APIVendor, f\"不支持的厂商: {vendor}\"\n",
        "            assert vendor.config['suported_models'], f\"厂商没有支持的模型: {vendor}\"\n",
        "        else:\n",
        "            vendor = APIVendor.DEEPSEEK\n",
        "\n",
        "        self.vendor = vendor\n",
        "        self.supported_models = vendor.config['suported_models']\n",
        "        config = self.vendor.config\n",
        "\n",
        "        # 合并配置，允许通过kwargs覆盖默认配置\n",
        "        init_params = {\n",
        "            'api_key': config['api_key'],\n",
        "            'base_url': config['base_url']\n",
        "        }\n",
        "        init_params.update(kwargs)\n",
        "\n",
        "        super().__init__(**init_params)\n",
        "\n",
        "        # 用我们的自定义实现替换chat资源\n",
        "        self.chat = CustomChat(self)\n",
        "\n",
        "# 默认配置\n",
        "DEFAULT_TEXT_LLM = LLMClient()  # model='deepseek-chat'\n",
        "DEFAULT_VISION_LLM = LLMClient(vendor=APIVendor.ALIYUN)  # model='qwen-vl-plus'\n",
        "DEFAULT_AUDIO_LLM = LLMClient(vendor=APIVendor.ALIYUNAUDIO)  # model='qwen-audio-turbo-latest'\n",
        "DEFAULT_VIDEO_LLM= LLMClient(vendor=APIVendor.ALIYUNVIDEO)"
      ],
      "metadata": {
        "id": "zCMk6zsgOUYd"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import currentframe\n",
        "\"\"\"\n",
        "语言模型编程 (LMP) - 单文件实现\n",
        "\n",
        "本模块提供了一个简洁的框架，用于与语言模型 (LLM) 进行交互，\n",
        "例如 OpenAI 的 GPT 系列（包括 GPT-4o 等多模态模型）或其他模型如 DeepSeek。\n",
        "它通过装饰器将 LLM 调用简化为 Python 函数调用。\n",
        "\n",
        "主要特性:\n",
        "- @simple: 用于函数返回简单文本提示，期望纯文本输出的装饰器。\n",
        "- @structured: 用于函数返回提示，期望 JSON 输出（内置自动修复）的装饰器。\n",
        "- 多模态输入: 通过返回列表中包含 ('image', data) 元组的方式支持文本和图像输入（URL、本地路径、Base64）。\n",
        "- 流式输出: 在装饰后的函数调用后附加 `.stream()` 以获取基于生成器的流式响应。\n",
        "- 批量处理: 附加 `.batch()` 以并发处理多个输入。\n",
        "- 灵活配置: 在装饰器级别设置默认模型、系统消息、API 参数，或在每次调用时覆盖。\n",
        "- 自动客户端选择: 根据模型名称启发式地选择合适的客户端（例如，为视觉模型选择 OpenAI 客户端）。\n",
        "\n",
        "基本用法:\n",
        "\n",
        "from lmp import simple, structured # 假设此文件保存为 lmp.py\n",
        "\n",
        "# 纯文本示例\n",
        "@simple(model='deepseek-chat')\n",
        "def greet(name: str):\n",
        "    # 函数文档字符串可用作系统消息\n",
        "    '''你是一个友好的助手。'''\n",
        "    return f\"向 {name} 说声你好。\"\n",
        "\n",
        "print(greet(\"世界\"))\n",
        "\n",
        "# 多模态示例 (需要配置 OpenAI 客户端)\n",
        "@structured(model='gpt-4o')\n",
        "def analyze_image(image_path: str, focus: str):\n",
        "    '''分析图像并返回 JSON。'''\n",
        "    return [\n",
        "        (\"image\", image_path), # 图像输入\n",
        "        f\"分析这张图片，重点关注 {focus}。返回包含 'description' 和 'details' 键的 JSON。\" # 文本提示\n",
        "    ]\n",
        "\n",
        "# 确保设置了 OPENAI_API_KEY 或已配置客户端\n",
        "# image_analysis = analyze_image(\"path/to/image.jpg\", \"主要主体\")\n",
        "\n",
        "# 多轮会话上下文的 lmp 示例\n",
        "@simple(model='deepseek-chat')\n",
        "def query_rewrite(query: str):\n",
        "    '''你是一个问题改写助手，通过上下文对用户问题进行提炼改写，禁止出现代词，不要多余废话。'''\n",
        "    return [\n",
        "        [\"告诉我关于秦始皇的事情\"],\n",
        "        [\"秦始皇是中国历史上第一个皇帝，他统一了六国并建立了秦朝。\"],\n",
        "        [query]\n",
        "    ]\n",
        "# 流式输出示例\n",
        "for chunk in query_rewrite.stream(\"他有哪些主要成就？\"):\n",
        "    print(chunk, end=\"\", flush=True)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from functools import wraps\n",
        "from typing import Any, Dict, List, Union, Iterator, Callable, Tuple\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from openai import OpenAI\n",
        "from json_repair import repair_json # 用于修复可能格式错误的 JSON\n",
        "import base64 # 用于图像 Base64 编码/解码\n",
        "import mimetypes # 用于猜测文件类型\n",
        "import os # 用于环境变量和文件路径操作\n",
        "import logging # 用于日志记录\n",
        "\n",
        "\n",
        "# 配置基本日志记录\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "log = logging.getLogger(__name__) # 获取日志记录器\n",
        "\n",
        "# ================== 工具函数 ==================\n",
        "def _format_audio_data(data: Union[str, bytes]) -> Dict[str, Any]:\n",
        "    \"\"\"将图像 URL、本地路径或字节数据格式化为 API 兼容的字典\"\"\"\n",
        "    url = \"\"\n",
        "    detail = \"auto\"\n",
        "\n",
        "    if isinstance(data, str):\n",
        "        if data.startswith((\"http://\", \"https://\")):\n",
        "            url = data\n",
        "        elif data.startswith(\"data:image/\"):\n",
        "            url = data\n",
        "        elif os.path.exists(data):\n",
        "            try:\n",
        "                with open(data, \"rb\") as image_file:\n",
        "                    b64_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "                mime_type, _ = mimetypes.guess_type(data)\n",
        "                mime_type = mime_type or \"image/png\"\n",
        "                if not mime_type.startswith(\"image/\"):\n",
        "                    raise ValueError(f\"无效的文件类型: {mime_type}\")\n",
        "                url = f\"data:{mime_type};base64,{b64_data}\"\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"处理本地图像文件 '{data}' 时出错: {e}\") from e\n",
        "        else:\n",
        "            # 新增：假设无前缀的字符串是 Base64 数据，添加默认 PNG 前缀\n",
        "            try:\n",
        "                # 简单验证是否可能是 Base64（检查长度和常见字符）\n",
        "                if len(data) > 50 and all(c in \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=\" for c in data[:50]):\n",
        "                    url = f\"data:image/png;base64,{data}\"\n",
        "                else:\n",
        "                    raise ValueError(f\"无效的图像字符串: 不是 URL、数据 URI 或现有文件路径: '{data[:100]}...'\")\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"无法解析图像数据: {e}\")\n",
        "    elif isinstance(data, bytes):\n",
        "        b64_data = base64.b64encode(data).decode('utf-8')\n",
        "        url = f\"data:image/png;base64,{b64_data}\"\n",
        "    else:\n",
        "        raise TypeError(f\"不支持的图像数据类型: {type(data)}\")\n",
        "\n",
        "    return url\n",
        "def _format_video_data(data: Union[str, bytes]) -> Dict[str, Any]:\n",
        "    \"\"\"将图像 URL、本地路径或字节数据格式化为 API 兼容的字典\"\"\"\n",
        "    url = \"\"\n",
        "    detail = \"auto\"\n",
        "\n",
        "    if isinstance(data, str):\n",
        "        if data.startswith((\"http://\", \"https://\")):\n",
        "            url = data\n",
        "\n",
        "        else:\n",
        "          raise TypeError(f\"非合法的视频链接: {type(data)}\")\n",
        "\n",
        "    else:\n",
        "        raise TypeError(f\"不支持的视频数据类型: {type(data)}\")\n",
        "\n",
        "    return {\"url\": url, \"detail\": detail}\n",
        "\n",
        "def _format_image_data(data: Union[str, bytes]) -> Dict[str, Any]:\n",
        "    \"\"\"将图像 URL、本地路径或字节数据格式化为 API 兼容的字典\"\"\"\n",
        "    url = \"\"\n",
        "    detail = \"auto\"\n",
        "\n",
        "    if isinstance(data, str):\n",
        "        if data.startswith((\"http://\", \"https://\")):\n",
        "            url = data\n",
        "        elif data.startswith(\"data:image/\"):\n",
        "            url = data\n",
        "        elif os.path.exists(data):\n",
        "            try:\n",
        "                with open(data, \"rb\") as image_file:\n",
        "                    b64_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "                mime_type, _ = mimetypes.guess_type(data)\n",
        "                mime_type = mime_type or \"image/png\"\n",
        "                if not mime_type.startswith(\"image/\"):\n",
        "                    raise ValueError(f\"无效的文件类型: {mime_type}\")\n",
        "                url = f\"data:{mime_type};base64,{b64_data}\"\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"处理本地图像文件 '{data}' 时出错: {e}\") from e\n",
        "        else:\n",
        "            # 新增：假设无前缀的字符串是 Base64 数据，添加默认 PNG 前缀\n",
        "            try:\n",
        "                # 简单验证是否可能是 Base64（检查长度和常见字符）\n",
        "                if len(data) > 50 and all(c in \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=\" for c in data[:50]):\n",
        "                    url = f\"data:image/png;base64,{data}\"\n",
        "                else:\n",
        "                    raise ValueError(f\"无效的图像字符串: 不是 URL、数据 URI 或现有文件路径: '{data[:100]}...'\")\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"无法解析图像数据: {e}\")\n",
        "    elif isinstance(data, bytes):\n",
        "        b64_data = base64.b64encode(data).decode('utf-8')\n",
        "        url = f\"data:image/png;base64,{b64_data}\"\n",
        "    else:\n",
        "        raise TypeError(f\"不支持的图像数据类型: {type(data)}\")\n",
        "\n",
        "    return {\"url\": url, \"detail\": detail}\n",
        "\n",
        "def _format_content_parts(parts: List[Union[str, tuple]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"将字符串和 ('类型', 数据) 元组的混合列表转换为 API 兼容的字典列表\"\"\"\n",
        "    formatted_content: List[Dict[str, Any]] = []\n",
        "\n",
        "    # 检查是否存在多模态内容\n",
        "    has_multimodal = any(isinstance(part, tuple) and part[0] != 'text' for part in parts)\n",
        "\n",
        "    for part in parts:\n",
        "        # 处理文本部分\n",
        "        if isinstance(part, str):\n",
        "            # 如果有其他多模态内容，文本部分需要明确类型\n",
        "            if has_multimodal:\n",
        "                formatted_content.append({\"type\": \"text\", \"text\": part})\n",
        "            else:\n",
        "                formatted_content.append(part)\n",
        "            continue\n",
        "\n",
        "        # 处理元组部分\n",
        "        if not (isinstance(part, tuple) and len(part) == 2):\n",
        "            raise ValueError(f\"不支持的内容部分: {type(part)}。应为 str 或 ('类型', 数据) 元组。\")\n",
        "\n",
        "        part_type, part_data = part\n",
        "        if part_type == \"image\":\n",
        "            try:\n",
        "                formatted_content.append({\"type\": \"image_url\", \"image_url\": _format_image_data(part_data)})\n",
        "            except (ValueError, TypeError) as e:\n",
        "                log.error(f\"跳过无效的图像数据: {e}\")\n",
        "        else:\n",
        "            log.warning(f\"内容中包含不支持的元组类型 '{part_type}'。已忽略。\")\n",
        "        if part_type == \"audio\":\n",
        "            try:\n",
        "                formatted_content.append({\"type\": \"audio\", \"audio\": _format_audio_data(part_data)})\n",
        "            except (ValueError, TypeError) as e:\n",
        "                log.error(f\"跳过无效的图像数据: {e}\")\n",
        "        else:\n",
        "            log.warning(f\"内容中包含不支持的元组类型 '{part_type}'。已忽略。\")\n",
        "        if part_type == \"video\":\n",
        "            try:\n",
        "                formatted_content.append({\"type\": \"video_url\", \"video_url\": _format_video_data(part_data)})\n",
        "            except (ValueError, TypeError) as e:\n",
        "                log.error(f\"跳过无效的图像数据: {e}\")\n",
        "        else:\n",
        "            log.warning(f\"内容中包含不支持的元组类型 '{part_type}'。已忽略。\")\n",
        "\n",
        "    return formatted_content\n",
        "\n",
        "def extract_content(response, vendor):\n",
        "    if vendor == APIVendor.ALIYUNAUDIO:\n",
        "        return response.output.choices[0].message.content[0]['text']\n",
        "\n",
        "    else:\n",
        "        return response.choices[0].message.content\n",
        "def get_stream_out(response_stream, vendor):\n",
        "    if vendor == APIVendor.ALIYUNAUDIO:\n",
        "        filtered_chunks = (chunk for chunk in response_stream\n",
        "                           if 'output' in chunk\n",
        "                           and 'choices' in chunk['output']\n",
        "                           and len(chunk['output']['choices']) > 0\n",
        "                           and 'message' in chunk['output']['choices'][0]\n",
        "                           and 'content' in chunk['output']['choices'][0]['message']\n",
        "                           and len(chunk['output']['choices'][0]['message']['content']) > 0)\n",
        "\n",
        "        # 直接从筛选后的流中产生 (yield) 文本块\n",
        "        for chunk in filtered_chunks:\n",
        "            # 提取文本内容\n",
        "            delta_content  = chunk.output.choices[0].message.content[0]['text']\n",
        "            if delta_content :\n",
        "                yield delta_content\n",
        "    else :\n",
        "        for chunk in response_stream:\n",
        "            delta_content = chunk.choices[0].delta.content\n",
        "            print(delta_content)\n",
        "            if delta_content:  # 仅当有新内容时才 yield\n",
        "                yield delta_content\n",
        "\n",
        "def build_messages(\n",
        "    result: Union[str, List[Union[str, Tuple]]],\n",
        "    system_message: str,\n",
        "    doc: str = None\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"构建最终用于 API 调用的消息字典列表\n",
        "\n",
        "    参数:\n",
        "        result: 可以是单个字符串(用户消息)或列表(多轮对话消息对)\n",
        "        system_message: 系统消息\n",
        "        doc: 函数文档字符串，可作为备用系统消息\n",
        "\n",
        "    返回:\n",
        "        格式化的消息列表，包含角色和内容\n",
        "\n",
        "    规则:\n",
        "        1. 消息内容必须是字符串或 ('类型', 数据) 元组，字符串默认是 \"text\"类型缺省的内容元组。\n",
        "        2. 其他类型当前仅支持(“image”, img_url), img_url是图像的链接地址或者 base64 编码的图片\n",
        "        3. 若 result 是单字符串： 系统消息 + 第一轮纯文本用户消息\n",
        "        4. 若 result 是单层列表： 系统消息 + 第一轮包含多模态数据的用户消息，比如[\"请描述图片内容\", (\"image\", image_url)]\n",
        "        5. 若 result 是双层列表[[]]: 系统消息 + 成对的用户/助手消息 + 最新一轮用户消息, 列表中的消息必须是奇数个元素\n",
        "    \"\"\"\n",
        "    messages: List[Dict[str, Any]] = []\n",
        "    effective_system_message = doc or system_message or \"你是一个有用的助手。\"\n",
        "\n",
        "    if isinstance(result, str):\n",
        "        # 单个字符串结果成为用户消息内容\n",
        "        messages.append({\"role\": \"system\", \"content\": effective_system_message})\n",
        "        messages.append({\"role\": \"user\", \"content\": result})\n",
        "    elif isinstance(result, (list, tuple)):\n",
        "        processed_list = list(result)\n",
        "\n",
        "        # 检查是否为双层列表（对话历史）\n",
        "        if all(isinstance(item, (list, tuple)) for item in processed_list):\n",
        "            # 双层列表处理：系统消息 + 对话历史 + 最新用户消息\n",
        "            messages.append({\"role\": \"system\", \"content\": effective_system_message})\n",
        "\n",
        "            # 确保消息数量为奇数\n",
        "            if len(processed_list) % 2 == 0:\n",
        "                raise ValueError(\"双层列表中的消息数量必须是奇数，包含系统消息后形成完整的对话轮次\")\n",
        "\n",
        "            for i, msg in enumerate(processed_list):\n",
        "                if not isinstance(msg, (str, tuple, list)):\n",
        "                    raise ValueError(f\"消息必须是字符串、元组或列表，得到: {type(msg)}\")\n",
        "\n",
        "                # 格式化消息\n",
        "                if isinstance(msg, list):\n",
        "                    content = _format_content_parts(msg)\n",
        "                else:\n",
        "                    content = _format_content_parts([msg])\n",
        "\n",
        "                # 确定角色：奇数索引为用户，偶数索引为助手\n",
        "                role = \"user\" if i % 2 == 0 else \"assistant\"\n",
        "                # print(f\"message {i}: {msg}, role: {role}, content: {content}\")\n",
        "                # 确保内容格式正确，直接使用格式化后的内容\n",
        "                messages.append({\"role\": role, \"content\": content[0] if isinstance(content, list) and len(content) == 1 else content})\n",
        "        else:\n",
        "            # 单层列表处理：系统消息 + 用户消息\n",
        "            messages.append({\"role\": \"system\", \"content\": effective_system_message})\n",
        "\n",
        "            # 格式化用户消息\n",
        "            user_content = _format_content_parts(processed_list)\n",
        "            messages.append({\"role\": \"user\", \"content\": user_content})\n",
        "    else:\n",
        "        raise ValueError(f\"被装饰的函数必须返回 str 或 list/tuple。得到: {type(result)}\")\n",
        "\n",
        "    # 验证最终消息结构\n",
        "    if not all(isinstance(m, dict) and 'role' in m and 'content' in m for m in messages):\n",
        "        log.error(f\"内部错误: build_messages 创建了无效的消息结构: {messages}\")\n",
        "        raise TypeError(\"未能构建有效的消息字典。\")\n",
        "\n",
        "    return messages\n",
        "\n",
        "def prepare_api_params(\n",
        "    default_params: Dict[str, Any], # 装饰器设置的默认参数\n",
        "    api_params: Dict[str, Any],     # 单次调用时传入的参数\n",
        "    model: str,                     # 由装饰器确定的最终模型名称 (必需)\n",
        "    stream: bool                    # 是否为流式调用\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"合并默认参数和调用特定参数\"\"\"\n",
        "    final_params = default_params.copy()\n",
        "    final_params.update(api_params) # 调用特定参数覆盖默认值\n",
        "    final_params[\"model\"] = model # 确保设置了模型\n",
        "    final_params[\"stream\"] = stream # 确保 stream 参数与调用类型匹配\n",
        "    return final_params\n",
        "\n",
        "def call_llm(client: OpenAI, messages: List[Dict[str, Any]], params: Dict[str, Any]):\n",
        "    \"\"\"执行实际的 LLM API 调用\"\"\"\n",
        "    if client is None:\n",
        "        raise ConnectionError(\"LLM 客户端未配置或初始化失败。\")\n",
        "    try:\n",
        "        model = params.get('model')\n",
        "        if not model:\n",
        "            raise ValueError(\"params中必须包含model参数\")\n",
        "        # 记录调试信息\n",
        "        log.debug(f\"调用 LLM API: 模型={model}, 流式={params.get('stream', False)}, 参数={ {k:v for k,v in params.items() if k not in ['stream', 'model']} }, 消息数={len(messages)}\")\n",
        "\n",
        "\n",
        "        response =client.chat.completions.create(messages=messages, **params)\n",
        "\n",
        "\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        log.error(f\"LLM API 调用失败: 模型={model}, 错误={e}\", exc_info=True)\n",
        "        if hasattr(e, 'response') and hasattr(e.response, 'text'):\n",
        "            e.response_content = e.response.text\n",
        "        raise\n",
        "\n",
        "\n",
        "# ================== 装饰器逻辑 ==================\n",
        "\n",
        "def _create_lmp_decorator(\n",
        "    is_structured: bool = False, # 是否期望结构化输出 (JSON)\n",
        "    llm_client: OpenAI = None,  # 是否为该装饰器指定了特定客户端\n",
        "    model: str = None,          # 是否为该装饰器指定了特定模型\n",
        "    system_message: str = None, # 是否显式指定了系统消息 (None 则依赖文档字符串)\n",
        "    **default_params          # 传递给 API 的默认参数 (如 temperature)\n",
        ") -> Callable:\n",
        "    \"\"\"创建实际装饰器的工厂函数\"\"\"\n",
        "\n",
        "    if not llm_client:\n",
        "        raise ValueError(\"必须提供有效的LLM客户端\")\n",
        "    if not model:\n",
        "        raise ValueError(\"必须指定模型名称\")\n",
        "\n",
        "    def decorator(func: Callable) -> Callable:\n",
        "        \"\"\"实际的装饰器函数\"\"\"\n",
        "        # 获取被装饰函数的文档字符串，作为潜在的系统消息来源\n",
        "        func_docstring = func.__doc__.strip() if func.__doc__ else None\n",
        "\n",
        "        @wraps(func) # 保留原函数的元信息 (名称, 文档字符串等)\n",
        "        def call_normal(*args, **kwargs) -> Union[str, Dict[str, Any]]:\n",
        "            \"\"\"处理标准的 (非流式) LLM 调用\"\"\"\n",
        "            # 从 kwargs 中弹出 api_params (如果存在)\n",
        "            api_params_override = kwargs.pop(\"api_params\", {})\n",
        "\n",
        "            # 调用用户定义的函数获取结果 (提示或消息列表)\n",
        "            func_result = func(*args, **kwargs)\n",
        "            messages = build_messages(func_result, system_message, func_docstring)\n",
        "\n",
        "            # 构建最终的消息列表，可能使用函数文档字符串作为系统消息\n",
        "\n",
        "            # 准备最终的 API 参数\n",
        "            final_params = prepare_api_params(\n",
        "                default_params, api_params_override, model=model, stream=False # 非流式\n",
        "            )\n",
        "            # 执行 API 调用\n",
        "            response = call_llm(llm_client, messages, final_params)\n",
        "            current_vendor = llm_client.vendor\n",
        "            # 提取响应内容\n",
        "\n",
        "            content = extract_content(response, current_vendor)\n",
        "\n",
        "\n",
        "            #content=response.output.choices[0].message.content[0]['text']\n",
        "            #content =response.choices[0].message.content\n",
        "            if is_structured:  # 如果期望 JSON 输出\n",
        "              try:\n",
        "                    # 尝试修复并解析 JSON\n",
        "                return repair_json(content, return_objects=True)  # return_objects=True 更稳健\n",
        "              except Exception as parse_error:\n",
        "                log.warning(f\"未能将 LLM 响应解析为 JSON: {parse_error}。返回原始内容:\\n{content}\")\n",
        "                return content  # 解析失败则返回原始字符串\n",
        "            else:  # 如果期望纯文本输出\n",
        "              return content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        def call_stream(*args, **kwargs) -> Iterator[str]:\n",
        "            \"\"\"处理流式 LLM 调用，产生文本块\"\"\"\n",
        "            api_params_override = kwargs.pop(\"api_params\", {})\n",
        "            func_result = func(*args, **kwargs)\n",
        "            messages = build_messages(func_result, system_message, func_docstring)\n",
        "            final_params = prepare_api_params(\n",
        "                default_params, api_params_override, model=model, stream=True  # 流式\n",
        "            )\n",
        "            vendor=llm_client.vendor\n",
        "            # 获取流式响应\n",
        "            response_stream = call_llm(llm_client, messages, final_params)\n",
        "            yield from get_stream_out(response_stream, vendor)\n",
        "\n",
        "        def call_batch(*batch_inputs: Union[Any, List[Any]], max_workers: int = 10, **batch_kwargs) -> Iterator[Union[str, Dict[str, Any], Exception]]:\n",
        "            \"\"\"并发处理多个输入的批量调用\"\"\"\n",
        "            # 标准化输入: 接受单个列表或多个参数作为输入项\n",
        "            if len(batch_inputs) == 1 and isinstance(batch_inputs[0], (list, tuple)):\n",
        "                all_inputs = batch_inputs[0]\n",
        "            else:\n",
        "                all_inputs = list(batch_inputs) # 将参数元组转为列表\n",
        "\n",
        "            log.info(f\"开始对 {len(all_inputs)} 个输入进行批量处理 (最大工作线程数={max_workers})\")\n",
        "            results = [None] * len(all_inputs) # 预分配结果列表以保持顺序\n",
        "            futures = {} # 用于将 future 映射回原始索引\n",
        "\n",
        "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "                for i, input_item in enumerate(all_inputs):\n",
        "                    # 准备 call_normal 的参数: 处理单个参数 vs 参数元组/列表\n",
        "                    call_args = input_item if isinstance(input_item, (list, tuple)) else [input_item]\n",
        "                    # 提交任务到线程池\n",
        "                    future = executor.submit(call_normal, *call_args, **batch_kwargs)\n",
        "                    futures[future] = i # 存储 future 到索引的映射\n",
        "\n",
        "                # 当任务完成时获取结果\n",
        "                for future in as_completed(futures):\n",
        "                    idx = futures[future] # 获取原始索引\n",
        "                    try:\n",
        "                        result = future.result() # 获取任务结果\n",
        "                        results[idx] = result\n",
        "                        log.debug(f\"批量处理项 {idx} 成功完成。\")\n",
        "                    except Exception as e:\n",
        "                        log.error(f\"处理索引 {idx} 的批量项时出错: {e}\", exc_info=True)\n",
        "                        results[idx] = e # 在结果中存储异常对象\n",
        "\n",
        "            # 按原始输入顺序产生结果\n",
        "            yield from results\n",
        "\n",
        "\n",
        "        # 将 stream 和 batch 方法附加到主装饰函数上\n",
        "        call_normal.stream = call_stream\n",
        "        call_normal.batch = call_batch\n",
        "        return call_normal # 返回包装后的主函数\n",
        "\n",
        "    return decorator # 返回配置好的装饰器\n",
        "\n",
        "\n",
        "# ================== 公开的装饰器 ==================\n",
        "\n",
        "def simple(\n",
        "    llm_client: OpenAI = DEFAULT_TEXT_LLM,\n",
        "    model: str = \"deepseek-chat\",\n",
        "    system_message: str = None,\n",
        "    **default_params\n",
        ") -> Callable:\n",
        "    \"\"\"\n",
        "    用于期望简单文本输出的 LLM 函数的装饰器。\n",
        "\n",
        "    Args:\n",
        "        llm_client: 覆盖默认客户端 (OpenAI 或其他)。\n",
        "        model: 指定模型 (例如 'gpt-4o', 'deepseek-chat')。如果客户端未指定，模型会决定客户端选择。\n",
        "        system_message: 显式系统消息 (覆盖函数文档字符串)。\n",
        "        **default_params: 默认 API 参数 (例如 temperature=0.7)。\n",
        "\n",
        "    Returns:\n",
        "        带有 `.stream()` 和 `.batch()` 方法的装饰后函数。\n",
        "    \"\"\"\n",
        "    return _create_lmp_decorator(\n",
        "        is_structured=False, # 非结构化输出\n",
        "        llm_client=llm_client,\n",
        "        model=model,\n",
        "        system_message=system_message,\n",
        "        **default_params\n",
        "    )\n",
        "\n",
        "def structured(\n",
        "    llm_client: OpenAI = DEFAULT_TEXT_LLM,\n",
        "    model: str = \"deepseek-chat\",\n",
        "    system_message: str = None,\n",
        "    **default_params\n",
        ") -> Callable:\n",
        "    \"\"\"\n",
        "    用于期望 JSON 输出 (会尝试修复) 的 LLM 函数的装饰器。\n",
        "\n",
        "    Args:\n",
        "        llm_client: 覆盖默认客户端。\n",
        "        model: 指定模型。如果客户端未指定，模型会决定客户端选择。\n",
        "        system_message: 显式系统消息 (覆盖函数文档字符串)。\n",
        "        **default_params: 默认 API 参数。\n",
        "\n",
        "    Returns:\n",
        "        装饰后函数，返回解析后的 JSON (或出错时返回原始字符串)，\n",
        "        带有 `.stream()` (产生原始文本块) 和 `.batch()` 方法。\n",
        "    \"\"\"\n",
        "    return _create_lmp_decorator(\n",
        "        is_structured=True, # 结构化输出\n",
        "        llm_client=llm_client,\n",
        "        model=model,\n",
        "        system_message=system_message,\n",
        "        **default_params\n",
        "    )"
      ],
      "metadata": {
        "id": "wW5uq9TvOX23"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from deepseek_tokenizer import ds_token\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import logging\n",
        "\n",
        "class DocumentChunkExtractor:\n",
        "    def __init__(self, model_name=\"tomaarsen/static-similarity-mrl-multilingual-v1\"):\n",
        "        \"\"\"初始化文档块提取器\n",
        "\n",
        "        Args:\n",
        "            model_name: 使用的向量模型名称，默认使用static-similarity-mrl-multilingual-v1\n",
        "        \"\"\"\n",
        "        # 静态向量模型，cpu 运行 ms 级,模型大小 400Mb，加载耗时 7s\n",
        "        self.embed_model = SentenceTransformer(model_name)\n",
        "\n",
        "    def extract_important_chunks(self, doc_text: str, max_chunks: int = 20, chunk_size: int = 128) -> list[str]:\n",
        "        \"\"\"提取文档中最重要的文本块\n",
        "\n",
        "        Args:\n",
        "            doc_text: 输入文档文本，非空字符串\n",
        "            max_chunks: 抽取后返回的最大块数，必须大于等于4\n",
        "            chunk_size: 文本块Token大小，必须大于0\n",
        "\n",
        "        Returns:\n",
        "            list[str]: 提取的重要文本块列表\n",
        "\n",
        "        Raises:\n",
        "            ValueError: 如果输入参数不合法\n",
        "        \"\"\"\n",
        "\n",
        "        # 参数校验\n",
        "        if not isinstance(doc_text, str) or not doc_text.strip():\n",
        "            raise ValueError(\"doc_text must be a non-empty string\")\n",
        "        if not isinstance(max_chunks, int) or max_chunks < 4:\n",
        "            raise ValueError(\"max_chunks must be an integer >= 4\")\n",
        "        if not isinstance(chunk_size, int) or chunk_size <= 0:\n",
        "            raise ValueError(\"chunk_size must be a positive integer\")\n",
        "\n",
        "        try:\n",
        "            # 分块并生成嵌入\n",
        "            chunks = self._split_text(doc_text, chunk_size)\n",
        "            if len(chunks) <= max_chunks:\n",
        "                logging.info(f\"文档已分割为{len(chunks)}个块(<=最大块数)，返回完整文本\")\n",
        "                return [doc_text]\n",
        "\n",
        "            logging.info(f\"文档已分割为{len(chunks)}个块，正在生成嵌入向量\")\n",
        "            embeddings = list(self.embed_model.encode(chunks))\n",
        "\n",
        "            # 使用 KMeans 聚类获取最近的样本索引\n",
        "            logging.info(f\"正在进行KMeans聚类，聚类数量为{max_chunks-4}\")\n",
        "            closest_indices = self._kmeans_clustering(embeddings, n_clusters=max_chunks-4)\n",
        "\n",
        "            # 获取首尾各两个块和聚类中心最近的文本块\n",
        "            important_indices = set(list(range(2)) + list(range(len(chunks)-2, len(chunks))))\n",
        "            cluster_indices = set([idx for idx in closest_indices if idx not in important_indices])\n",
        "            all_indices = sorted(list(important_indices | cluster_indices))\n",
        "\n",
        "            logging.info(f\"从文档中选择了{len(all_indices)}个重要文本块\")\n",
        "            return [chunks[idx] for idx in all_indices]\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"提取重要文本块时出错: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def _split_text(self, text, chunk_size=128):\n",
        "        \"\"\"将文本转换为 tokens，按 token 数量进行切分，然后将每个 token 切分块重新转换为汉字返回。\n",
        "\n",
        "        Args:\n",
        "            text: 输入的完整文本字符串\n",
        "            chunk_size: 每个块的最大 token 数量\n",
        "\n",
        "        Returns:\n",
        "            分块后的文本列表，每个元素为一个字符串\n",
        "        \"\"\"\n",
        "        # 将文本编码为 tokens\n",
        "        token_list = ds_token.encode(text)\n",
        "        # 按照指定的 chunk_size 将 token 切分\n",
        "        chunks_tokens = [token_list[i:i + chunk_size] for i in range(0, len(token_list), chunk_size)]\n",
        "        # 将每个 token 切分块重新解码为汉字并返回\n",
        "        chunks = [ds_token.decode(tokens) for tokens in chunks_tokens]\n",
        "        return chunks\n",
        "\n",
        "\n",
        "    def _kmeans_clustering(self, embeddings, n_clusters=16, random_state=0):\n",
        "        \"\"\"使用KMeans对文本块进行聚类，找到距离簇心最近的样本\n",
        "\n",
        "        Args:\n",
        "            embeddings: 文本块的向量表示\n",
        "            n_clusters: 聚类数量\n",
        "            random_state: 随机种子\n",
        "\n",
        "        Returns:\n",
        "            list: 最接近聚类中心的样本索引列表\n",
        "        \"\"\"\n",
        "        # 使用 KMeans 聚类算法\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(embeddings)\n",
        "        labels = kmeans.predict(embeddings)\n",
        "        cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "        # 为每个聚类找到最接近中心的样本\n",
        "        closest_indices = []\n",
        "        for cluster_idx in range(n_clusters):\n",
        "            # 获取当前聚类的所有样本索引\n",
        "            cluster_indices = [i for i, label in enumerate(labels) if label == cluster_idx]\n",
        "            # 计算当前聚类所有样本与聚类中心的相似度\n",
        "            similarities = cosine_similarity([cluster_centers[cluster_idx]], [embeddings[i] for i in cluster_indices])[0]\n",
        "            # 获取最相似样本的索引\n",
        "            closest_indices.append(cluster_indices[similarities.argmax()])\n",
        "\n",
        "        return closest_indices\n"
      ],
      "metadata": {
        "id": "19xbRDBtel1H"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# audio模型多轮上下文测试\n",
        "print(\"\\n========audio模型上下文测试=========\")\n",
        "@simple(llm_client=DEFAULT_AUDIO_LLM,model=\"qwen-audio-turbo-latest\")\n",
        "def describe_image(image_url: str) -> str:\n",
        "    \"\"\"你是一个音频分析员\"\"\"\n",
        "    return [\"请描述音频内容\", (\"audio\", image_url)]\n",
        "\n",
        "\n",
        "print(\"\\n流式调用:\")\n",
        "for chunk in describe_image.stream(\"https://dashscope.oss-cn-beijing.aliyuncs.com/audios/welcome.mp3\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqq2l-VrPAcd",
        "outputId": "7840e021-d039-4c96-fc77-419c5e29adec"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:内容中包含不支持的元组类型 'audio'。已忽略。\n",
            "WARNING:__main__:内容中包含不支持的元组类型 'audio'。已忽略。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========audio模型上下文测试=========\n",
            "\n",
            "流式调用:\n",
            "音频内容为：“欢迎使用阿里云”。"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 视觉模型多轮上下文测试\n",
        "print(\"\\n========视觉模型上下文测试=========\")\n",
        "@structured(llm_client=DEFAULT_VISION_LLM,model=\"qwen-vl-plus\")\n",
        "def describe_image(image_url: str) -> str:\n",
        "    \"\"\"你是一个图像分析员\"\"\"\n",
        "    return [\"请描述图片内容\", (\"image\", image_url)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for chunk in describe_image.stream(\"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiSggEIFPGlg",
        "outputId": "338ee1a6-5c1e-46ea-e6e1-630659d1de33"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:内容中包含不支持的元组类型 'image'。已忽略。\n",
            "WARNING:__main__:内容中包含不支持的元组类型 'image'。已忽略。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========视觉模型上下文测试=========\n",
            "\n",
            "这张\n",
            "这张照片\n",
            "照片展示\n",
            "展示了一位女士和\n",
            "了一位女士和一只狗在海滩\n",
            "一只狗在海滩上互动的场景\n",
            "上互动的场景。阳光从画面\n",
            "。阳光从画面右侧斜射过来\n",
            "右侧斜射过来，给整个场景\n",
            "，给整个场景增添了一份温暖的感觉\n",
            "增添了一份温暖的感觉。\n",
            "\n",
            "-\n",
            "。\n",
            "\n",
            "- 女士坐在沙滩\n",
            " 女士坐在沙滩上，穿着格\n",
            "上，穿着格子衬衫、黑色\n",
            "子衬衫、黑色裤子，并且戴着\n",
            "裤子，并且戴着一块手表。\n",
            "-\n",
            "一块手表。\n",
            "- 狗是一\n",
            " 狗是一只金毛犬\n",
            "只金毛犬（可能是拉布拉\n",
            "（可能是拉布拉多），它正\n",
            "多），它正伸出前爪与\n",
            "伸出前爪与女人的手相碰\n",
            "女人的手相碰或握手的动作，在\n",
            "或握手的动作，在这个过程中看起来非常\n",
            "这个过程中看起来非常开心和平静地\n",
            "开心和平静地享受着这一刻。\n",
            "享受着这一刻。\n",
            "  \n",
            "背景是平静\n",
            "\n",
            "  \n",
            "背景是平静的大海以及渐\n",
            "的大海以及渐变色天空的颜色\n",
            "变色天空的颜色变化，给人一种宁静\n",
            "变化，给人一种宁静而美好的感觉。\n",
            "而美好的感觉。整体氛围轻松愉悦\n",
            "整体氛围轻松愉悦, 似乎是在\n",
            ", 似乎是在一个晴朗的日子\n",
            "一个晴朗的日子度过美好时光的好\n",
            "度过美好时光的好地方！\n",
            "地方！"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 视觉模型多轮上下文测试\n",
        "print(\"\\n========视觉模型上下文测试=========\")\n",
        "@structured(llm_client=DEFAULT_VIDEO_LLM,model=\"qwen-vl-max-latest\")\n",
        "def describe_video(video_url: str) -> str:\n",
        "    \"\"\"你是一个视频分析员\"\"\"\n",
        "    return [\n",
        "        (\"video\", video_url),#支持base64编码\n",
        "        f\"\"\"\n",
        "          你是一位视频理解与信息结构化专家，请分析以下视频内容，并根据视频实际情况提取摘要信息，输出标准 JSON 格式，并根据类型完成以下任务：\n",
        "\n",
        "          1.为视频取一个标题，标题简短概述视频主题\n",
        "          2.描述视频内容，按照视频的时间顺序，完整的讲述视频中的内容\n",
        "          3.确定视频中出现的对象，按照时间顺序依次输出\n",
        "          4.视频发生的场景\n",
        "          5.视频中有文字就要尽可能提取出结构化的 Markdown 内容\n",
        "            视频中没文字可以不输出Markdown内容\n",
        "\n",
        "\n",
        "          请严格输出以下 JSON 格式内容，不要添加解释说明：\n",
        "              {{\n",
        "\n",
        "                \"title\": string,\n",
        "                \"summary\": string,\n",
        "                \"objects\": string[],\n",
        "                \"scene\": string,\n",
        "                \"markdown_content\": string | null\n",
        "              }}\n",
        "\n",
        "          示例 1：\n",
        "              {{\n",
        "\n",
        "                \"title\": \"两人进行篮球单挑\",\n",
        "                \"summary\": \"视频描述了两个运动员进行篮球单挑的场面，在一个回合中，球员A在进攻，球员B在防守，球员A在球员B的防守下投进了三分球\",\n",
        "                \"objects\": [\"球员A\", \"球员B\",\"篮球\"],\n",
        "                \"scene\": \"篮球场\",\n",
        "                \"markdown_content\": \"## 比赛信息\\n\\n- **球员A**：球衣号码45\\n- **球员B**：球衣号码23\\n- 比分：46：50\\n- 场地：新光篮球场\\n- 篮球品牌：Nike\"\n",
        "              }}\n",
        "\n",
        "\n",
        "              请开始处理视频，现在，不要给出任何解释性文本，请直接输出结果,不要加任何其他的字样如json:\n",
        "              （视频中没有文字严格不生成\"markdown_content\",视频中有文字就一定要生成\"markdown_content\"）\n",
        "              ：\n",
        "\n",
        "          \"\"\"\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "result=describe_video(\"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241115/cqqkru/1.mp4\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSLWsvuOQp_l",
        "outputId": "89044a7b-d97b-493d-b3dd-438145543aba"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:内容中包含不支持的元组类型 'video'。已忽略。\n",
            "WARNING:__main__:内容中包含不支持的元组类型 'video'。已忽略。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========视觉模型上下文测试=========\n",
            "{'title': '微笑女孩的户外瞬间', 'summary': '视频展示了一位短发女孩在户外微笑的画面。她穿着粉色毛衣和白色内搭，佩戴着简单的项链。女孩面带微笑，眼神明亮，表情自然愉悦，时而微微点头，展现出轻松愉快的心情。', 'objects': ['女孩', '粉色毛衣', '白色内搭', '项链'], 'scene': '户外建筑前', 'markdown_content': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 视觉模型多轮上下文测试\n",
        "print(\"\\n========视觉模型上下文测试=========\")\n",
        "@structured(llm_client=DEFAULT_VIDEO_LLM,model=\"qwen-vl-plus\")\n",
        "def describe_video(image_url: str) -> str:\n",
        "    \"\"\"你是一个视频分析员\"\"\"\n",
        "    return [\"请描述图片内容\", (\"imageimage\", image_url)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "result=describe_video(\"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WIuWXqUSdGw",
        "outputId": "e68994bb-b0ab-4a3a-a99d-a8abf595ebda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:内容中包含不支持的元组类型 'imageimage'。已忽略。\n",
            "WARNING:__main__:内容中包含不支持的元组类型 'imageimage'。已忽略。\n",
            "WARNING:__main__:内容中包含不支持的元组类型 'imageimage'。已忽略。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========视觉模型上下文测试=========\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 视觉模型多轮上下文测试\n",
        "print(\"\\n========视觉模型上下文测试=========\")\n",
        "@structured(llm_client=DEFAULT_VISION_LLM,model=\"qwen-vl-plus\")\n",
        "def analyse_image(image_url: str) -> str:\n",
        "    \"\"\"你是一个图像分析员\"\"\"\n",
        "    return [\n",
        "        (\"image\", image_url),#支持base64编码\n",
        "        f\"\"\"\n",
        "          你是一位图像理解与信息结构化专家，请分析以下图像内容，并根据图像实际情况提取摘要信息，输出标准 JSON 格式。你需要自动判断图像类型，并根据类型完成以下任务：\n",
        "          1. 判断图像类型（如：“风景照”、“证件照”、“图表”、“文本图”、“插画”、“截图”、“实物照片”等）\n",
        "          2.为图像取一个标题\n",
        "          3，维图像生成一个描述摘要\n",
        "          4,描述图像中主体对象\n",
        "          5，场景描述\n",
        "          6，图片中有文字就要尽可能提取出结构化的 Markdown 内容\n",
        "            图片中没文字可以不输出Markdown内容\n",
        "\n",
        "\n",
        "          请严格输出以下 JSON 格式内容，不要添加解释说明：\n",
        "              {{\n",
        "                \"image_type\": string,\n",
        "                \"title\": string,\n",
        "                \"summary\": string,\n",
        "                \"objects\": string[],\n",
        "                \"scene\": string,\n",
        "                \"markdown_content\": string | null\n",
        "              }}\n",
        "\n",
        "          示例 1：文本图（产品介绍页面截图）\n",
        "              {{\n",
        "                \"image_type\": \"文本图\",\n",
        "                \"title\": \"AI产品功能介绍\",\n",
        "                \"summary\": \"该图为某AI平台产品功能页截图，包含智能问答、多模态数据处理、API 接入功能模块介绍及操作说明，内容结构清晰。\",\n",
        "                \"objects\": [\"标题栏\", \"图标\", \"文本段落\", \"功能区块\"],\n",
        "                \"scene\": \"产品介绍页面截图\",\n",
        "                \"markdown_content\": \"## 产品功能一览\\n\\n- 智能问答系统\\n- 多模态数据处理\\n- API 接入支持\\n\\n> 联系我们获取更多功能详情。\"\n",
        "              }}\n",
        "              示例 2：自然图（风景照片）\n",
        "              {{\n",
        "                \"image_type\": \"风景照\",\n",
        "                \"title\": \"雪山湖泊倒影图\",\n",
        "                \"summary\": \"图像展示一座雪山在湖面中的倒影，天空晴朗，色彩冷静宁静，构图对称。\",\n",
        "                \"objects\": [\"雪山\", \"湖泊\", \"天空\", \"倒影\"],\n",
        "                \"scene\": \"自然风光摄影\",\n",
        "                \"markdown_content\": null\n",
        "              }}\n",
        "\n",
        "              请开始处理图像，现在，不要给出任何解释性文本，请直接输出结果,不要加任何其他的字样如json:\n",
        "              （图像中没有文字严格不生成\"markdown_content\",图像中有文字就一定要生成\"markdown_content\"）\n",
        "              ：\n",
        "\n",
        "          \"\"\"\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "result=analyse_image(\"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_94pt-LfYkvP",
        "outputId": "41a8f209-d4f2-4de2-96d1-c57613993595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:内容中包含不支持的元组类型 'image'。已忽略。\n",
            "WARNING:__main__:内容中包含不支持的元组类型 'image'。已忽略。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========视觉模型上下文测试=========\n",
            "{'image_type': '实物照片', 'title': '海滩上的友好互动', 'summary': '这张图片展示了一个人和一只狗在沙滩上愉快地玩耍的温馨时刻，在夕阳下显得格外温暖和谐。', 'objects': ['女人', '金毛犬', '沙子', '海洋波浪'], 'scene': '海边休闲时光', 'markdown_content': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_txt='''import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 模拟供应链数据\n",
        "data = {\n",
        "    'date': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n",
        "    'raw_material_price': np.random.normal(100, 5, 100),\n",
        "    'demand': np.random.normal(200, 20, 100),\n",
        "    'transport_time': np.random.normal(2, 0.5, 100)  # 天\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df['predicted_demand'] = df['demand'] + np.random.normal(0, 5, 100)\n",
        "\n",
        "# 构建一个简单的预测模型\n",
        "X = df[['raw_material_price', 'transport_time']]\n",
        "y = df['predicted_demand']\n",
        "model = RandomForestRegressor(n_estimators=100)\n",
        "model.fit(X, y)\n",
        "\n",
        "# 预测未来30天的需求\n",
        "future_dates = pd.date_range(start=df['date'].max() + timedelta(days=1), periods=30, freq='D')\n",
        "future_data = pd.DataFrame({\n",
        "    'date': future_dates,\n",
        "    'raw_material_price': np.random.normal(100, 5, 30),\n",
        "    'transport_time': np.random.normal(2, 0.5, 30)\n",
        "})\n",
        "\n",
        "future_data['predicted_demand'] = model.predict(future_data[['raw_material_price', 'transport_time']])\n",
        "\n",
        "# 可视化\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['date'], df['predicted_demand'], label='历史预测需求')\n",
        "plt.plot(future_data['date'], future_data['predicted_demand'], label='未来预测需求', linestyle='--')\n",
        "plt.xlabel('日期')\n",
        "plt.ylabel('需求量')\n",
        "plt.title('供应链需求预测')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\"'''"
      ],
      "metadata": {
        "id": "vWdfgCt5dmRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@structured(llm_client=DEFAULT_TEXT_LLM,model=\"deepseek-chat\")\n",
        "def analyse_code(code_txt) -> str:\n",
        "    \"\"\"你是一位代码分析专家\"\"\"\n",
        "    return f\"\"\"\n",
        "    请根据提供的代码文本，生成一个合适的标题、摘要以及代码中的关键变量。适用于后续检索和快速理解代码内容。\n",
        "\n",
        "    标题 (title)：应包含代码语言，并简要概括代码的主要功能或用途，例如 \"Python - 图像处理脚本\" 或 \"JavaScript - REST API 服务器\"。\n",
        "    摘要 (summary)：概述代码的核心逻辑、关键功能模块和主要实现方法，避免逐行解释，但应涵盖整体结构和主要算法思路。\n",
        "    关键变量 (keywords)：提取代码中最重要的变量、至多 10 个，包括但不限于导入变量、类名、函数名称，核心参数配置、核心逻辑变量等。\n",
        "\n",
        "    请按照 JSON 格式 输出，确保格式规范，示例如下：\n",
        "    {{\n",
        "      \"title\": \"代码语言 - 数据分析工具\",\n",
        "      \"summary\": \"该 Python 脚本用于处理和分析 CSV 数据集，使用 Pandas 进行数据清洗，并利用 Matplotlib 绘制可视化图表。主要功能包括数据加载、缺失值处理、统计分析和趋势绘制。\",\n",
        "      \"keywords\": [\"dataframe\", \"csv_file\", \"analyze_data\", \"plot_trends\"]\n",
        "    }}\n",
        "\n",
        "    输入代码如下：\n",
        "    {code_txt}\n",
        "    现在，不要给出任何解释性文本，请直接输出结果（注意不要输出json这个词）:\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "print(analyse_code(code_txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HndUtC2lZwNw",
        "outputId": "e1130654-1393-4216-c561-23c7ba189376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'Python - 供应链需求预测分析', 'summary': '该 Python 脚本使用随机森林回归模型对供应链需求进行预测分析。首先生成模拟供应链数据（包括原材料价格、运输时间和需求），然后训练随机森林模型，预测未来30天的需求，并使用Matplotlib可视化历史预测和未来预测结果。', 'keywords': ['RandomForestRegressor', 'pd.DataFrame', 'future_dates', 'predicted_demand', 'raw_material_price', 'transport_time', 'model.fit', 'model.predict', 'plt.plot', 'plt.show']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content='''\n",
        "随着人类社会夜间活动的增加，各类光源在夜间环境中的广泛应用，造成的问题便是对夜间图像造成了新的干扰，耀斑。这一现象的原因源于镜头光学系统的物理结构特性。镜头通常由多个透镜元件组成，光线在穿过这些透镜时，尽管大部分能够直接透过，但仍有部分光线在透镜表面发生反射。这些反射光在镜头内部多次反射或散射后，最终到达图像传感器，形成光斑或辉光，尤其是当透镜元件数量增加时，这种内部反射和扩散效应会更加显著，从而导致更为明显的眩光现象。尽管现代光学镜头设计已通过多种技术手段在一定程度上缓解了辉光的影响，然而在实际拍摄场景中，镜头表面的灰尘、油渍、雨点、微小划痕，以及空气中的水雾或悬浮颗粒等，都会显著增强光线在镜头表面的散射与反射，进而加剧辉光现象的出现。\n",
        "特别是对于长期暴露在户外环境中的车载摄像头和监控摄像头，以及在日常使用中频繁受到磨损的手机摄像头，外界环境的影响往往难以避免。随着车载系统和监控设备广泛应用于自动驾驶和智能安防等领域，如何在成像后处理环节有效地抑制辉光现象，成为解决这一问题的更为实际和具有针对性的方法。\n",
        "大多数夜间图像增强算法以提高图像暗部区域的细节为核心目标。这些方法即使提升了图像的亮度，但经过我们的测试这种亮度提升会造成耀斑现象的加剧，导致更严重的图像破坏，在如今复杂的夜间图像场景中，去除耀斑的任务更加重要。\n",
        "目前去耀斑任务的难点一直在于缺少相关数据集，采集配对的耀斑图像和无耀斑图像是一件非常困难的事情，至今仍然没有真实收集的能够达到训练规模的数据集。一些人也发现了这个问题，whu等人，sun等人基于太阳光的特点生成了耀斑模拟，然而这些方法在与夜间复杂情况下的耀斑状况差别较大，对真实耀斑的处理效果并不好。Yuekun Da 等人先后提出了Flare7K和Flare7K++数据集，这是第一个针对夜间辉光生成的数据集，采用了对辉光完全手工合成，再叠加到真实图像上的想法，是目前与真实耀斑效较为接近的一种方法。然而其手工合成的辉光叠加到图像上往往效果不够自然，其割裂感往往成为一种较为学习的特征。这也就让其训练出的模型在真实图像中，面对柔和的夜间辉光时，往往起不到效果。\n",
        "我们提出了一种创新性的模拟耀斑的方法，让耀斑和真实世界中一样，由光源而发，先确定图像中的真实光源，在基于光源进行辉光模拟，而不是预先设定好耀斑的形式。耀斑的特性很大程度上应该由光源本身决定，于是我们首先对图像中光源的特性进行提取，光源的形状，亮度，颜色等。由这些因素限定耀斑的特性。这样生成的耀斑自然程度会比把耀斑直接添加到背景图像上高很多，更加符真实耀斑图像的特征学习难度。为了保证模拟的耀斑和真实的耀斑有着相同的物理规律，特性。我们采集了大概三百张的真实耀斑图像总结出了真实的可进行仿真的规律。为了充分模拟夜间耀斑的复杂多样性，基于这些规律我们不限定具体的值，只给定合理的范围，在合理的范围内随机设定各类参数。同时我们设计的，查找图像中可能光源的函数，允许一张图像中同时存在多个候选光源。我们会为每个光源分别生成辉光，这些操做可以保证我们数据集可以根据一定数量的夜间背景图生成大量的配对图像。\n",
        "辉光具有较强的局部性，我们发现Flare7K的有监督方法中部分图像处理效果不好的原因在于模型无法正确感知到辉光的位置。基于此我们想通过一个简单的阈值分割任务来引导模型学习辉光的位置，这种做法强化了模型学习耀斑位置的能力。我们设计了一全新的模型架构，采用了双路径解码器，每个路径的解码器对应着学习mask，和学习去除辉光的任务。由学习mask的结果以及特征图引导去耀斑学习任务。我们的数据集以及Flare7K等合成数据集可以较为轻易的获取到耀斑区域的mask，可以用于监督我们的mask部分的效果。我们提出了mask图引导的跳跃链接新方法，为此我们设计了一个特征激活融合模块。实验结果表明我们的方法对很多困难的辉光图有着较好的处理效果。\n",
        "本研究的主要贡献有四个方面：\n",
        "1.首先我们提出了首个从图像中定位光源，获取光源信息，生成辉光的方法。我们的方法适用于绝大部分场景下耀斑的模拟生成。生成的耀斑有很高的视觉真实性，符合真实耀斑的物理规律。同时支持单张图像中的的多光源分别生成。\n",
        "2.第一个完全基于夜间场景的大规模高质量耀斑配对数据集。我们收集了五百张真实有干净光源的夜间图像，这些图像作为原图，基于这些图像上的光源我们生成了耀斑模拟，我们对模拟的参数只设定上下界，来保证其处于在一个合理的范围区间，在这个区间内随机生成参数。通过组合可以实现单个光源生成大量特征有明显区别且合理的耀斑，结合单张图像多光源生成。我们基于这五百张原图生成了16000张高质量张配对辉光数据集。\n",
        "3.我们设计了一个针对于耀斑等这类局部图像损坏的修复模型。由简单的阈值分割任务进行引导，加强位置信息，提升去耀斑这类复杂任务的学习效果。创新性的网络架构设计可以保证增加较少参数的情况下完成阈值分割的学习和引导任务。\n",
        "4.我们改进了传统跳跃连接容易产生的信息冗余和信息失配的问题，我们采用了mask特征图引导连接的思想。为此设计了一特征激活融合模块，这个模块可以广泛应用于各类特征融合的任务。\n",
        "镜头光晕数据集。收集大规模的成对光晕污染图像和无光晕图像数据集依靠拍摄是非常难以实现的。为了解决这个问题，wu等人提出了第一个合成光晕数据集，包含2001张拍摄的光晕图和3000张模拟光晕图。为合成耀斑的思路做了很大的贡献，但其耀斑图像均来自于统一镜头，模拟的光晕图整体都是白光图，形式非常统一，与夜间的真实复杂情况差别较大。用其训练之后的模型对夜间耀斑的去除效果较差。\n",
        "yudakan等人基于 Adobe After Effects 中的插件手动合成了7000张耀斑图像，耀斑的形式非常丰富，且视觉真实性很高，其对耀斑成分的分析为耀斑的模拟提供了很大的指导。但其存在的问题是这种预设好的耀斑图直接添加到背景图像上会存在一些问题。首先是与背景的割裂感，耀斑图像无法根据光源的情况去选择，耀斑的特性和光源特性往往差别教导，这也就导致添加到背景图像上时会与原图有很强的割裂感，而真实图像中辉光往往和背景图之间是平滑的过度，这种割裂感特征往往会干扰模型的训练，削弱了模型在处理真实辉光的判断和处理能力。其次便是其叠加的方式会导致图像上的光源本身的信息被遮挡，往往真实场景中，光源附近的信息是我们需要的，这种遮挡会导致处理后的图像存在伪影，其在Flare7K++中有所改善处理，但其通过裁剪耀斑中亮块作为光源信息的方法和真实场景中有一定差距，导致其结果还是存在光源周围信息丢失的问题。其采用的背景图像是常规图像，这些图像整体较亮，会导致模拟后的辉光中离中心较远处亮度较暗的部分没有办法很好体现，这也就导致其结果对夜间图像中亮度不是很高的光雾的处理效果较差。\n",
        "\n",
        "Eino-Vi\n",
        "lle Talvala [15]等人较早对薄雾眩光进行了量化分析，并通过使用高频遮挡掩膜多次捕获图像，以估计和去除眩光。尽管此方法在薄雾眩光的处理上具有创新性，但其局限性在于对复杂场景中的强眩光效果有限。Wanyu Wu [16]等人设计了一种光源感知的盲去卷积模型（LBDN），结合光源空间位置掩膜和基于APSF（大气点扩散函数）的先验信息，用于估计多重散射图，但其对图像无辉光部分的破坏较大，会导致处理后的图像整体偏暗。Yeying Jin [18] 等人构建了一个未配对的夜间光晕和干净光源数据集，并提出了一种结合层分离网络与CycleGAN的双层设计。然而，其层分离网络在处理大范围光晕时效果不理想，且CycleGAN在训练过程中表现出较大的不稳定性，容易产生伪影，同时可能导致光源本身被遮挡或削弱的问题。Sharma 和 Tan [17]提出了一种基于相机响应函数（CRF）估计、频域分解和HDR成像的光效应抑制方法。然而，该方法在处理无色白光和强光晕时效果不佳，容易出现细节丢失和抑制不足的问题。Wu [19]等人提出了第一个针对辉光的数据集，生成了配对的有辉光和干净的图像，基于U-Net的pix2pix模型在其数据集上训练了一个去除眩光的网络。其训练的模型对光源的要求较高，很容易出现无法识别光源并无法消除辉光的问题，并且在夜间场景下效果不是很好。Yuekun Da[20,21]等手工合成辉光，并将其合成的辉光通过变形然后添加到背景图片上的方法制作了一个合成数据集，训练了一个端到端的辉光去除框架，其问题是在数据集处提到的对光源周围信息的抑制和对光雾较弱的处理能力。\n",
        "//辉光介绍\n",
        "\n",
        "近年来，许多研究者在根据光源模拟光源周围辉光效应时，采用了基于成像原理的方法，利用与光雾模拟类似的技术，使用（APSF, Axially Symmetric Point Spread Function）来描述理想点光源经过镜头系统后在图像平面上形成的光强分布。然而，我们在实验中发现，基于简单APSF生成的辉光与真实图像中的辉光效果之间存在明显的视觉差距,难以直接用于生成符合实际需求的高质量数据集。\n",
        "为解决上述问题，我们从真实包含辉光的图像中学习辉光特。我们收集了100张真实的辉光图像，这些图像的设计尽量简化了场景复杂性，仅包含光源本身及其周围的辉光效果，同时背景接近全黑。能够有效避免复杂背景对数据分析的干扰，使得后续的分析更加专注于辉光的本质特征。\n",
        "基于该数据集，我们进一步对真实图像中光源的亮度分布及其随角度和距离变化的衰减特性进行了深入研究。这种数据驱动的分析方法为更真实地重建辉光效应提供了可能，并为生成高质量的模拟数据集奠定了基础。辉光特性分析与模拟方法\n",
        "\t通过对真实辉光图像的研究，我们总结发现辉光效应可从以下两个角度进行整体建模和模拟：\n",
        "径向亮度衰减（光晕效应，Halo Effect）：\n",
        "\t辉光的第一部分表现为亮度随着距离光源中心的增加而呈现非线性下降。这种现象通常表现为光源周围的光晕效果，且亮度衰减的速率随距离的变化逐渐减缓。\n",
        "为了得到径向亮度的合理分布函数，我们从每张真实图像中提取辉光强度随光心距离变化的衰减曲线，并对其进行归一化处理以消除绝对亮度差异的影响。通过统计分析大量图像中的衰减曲线，我们提取并总结出了整体的平均衰减趋势，以此为基准进行模拟\n",
        "点扩散函数（PSF, Point Spread Function）相似的高斯衰减模型，对辉光随距离的亮度变化进行拟合与模拟。然而，与直接使用标准PSF函数不同，我们在模型中并不固定指数函数的衰减速率，而是仅对其在上、下边界进行了约束，同时我们采用了多种函数的混合 / 分段模型，将多类型衰减形函数（指数、高斯、多项式）拼接或加权叠加，并加入随机噪声的起伏以模拟真实光学衰减中更加复杂的效果；也就是说，我们先通过对大量图像的统计分析得到了一个合理的衰减速率范围，然后在此范围内随机选择衰减函数的类型以及不同的衰减速率，噪声形式。这样做的好处在于。符合整体衰减规律：通过分析大量真实光源辉光图像，已获取了一个真实可信的衰减速率范围，能够保证生成的辉光模拟数据拥有与实际物理过程相一致的整体趋势。增强生成数据的随机性与多样性：在每次生成辉光数据时，都可以在预先设定的上下界之间随机地选取衰减速率，从而得到不同梯度的衰减曲线。避免了一刀切的固定衰减参数，使生成的数据分布更加丰富，更有助于增强训练模型时间的泛化性环向亮度起伏（条纹效应，Streak Effect）：\n",
        "\t辉光的第二特性是沿环向的亮度起伏，通常表现为线状的耀斑或条纹（streaks），Streak（条形耀斑）是辉光的一种重要组成部分，具有与光晕相似的径向亮度衰减特性，但同时表现出显著的环向分布特性和亮度边界。我们同样对收集到的辉光图像进行定量分析，总结了Streak的以下特性：环向分布特性：\n",
        "Streak通常出现在特定角度，呈现条状分布，可分为以下两种典型形态：宽条状： 宽度较大，亮度高于周围光晕，透明度较高，衰减较为明显。细线状： 宽度较小，亮度非常高，透明度较低，衰减不显著。周围光晕的“缺口效应”：Streak的出现通常伴随着两侧光晕亮度的下降，形成类似“缺口”的区域。\n",
        "基于上述特性我们分别设计模拟两种streaks的形态，每次随机角度生成，引入径向噪声：为增强光线的丝状的真实感，我们在径向方向上加入丝状噪声。在Streak两侧光晕区域内引入亮度抑制函数，通过降低光晕亮度实现“缺口效应”的模拟。抑制程度与Streak的宽度和亮度相关联，以确保模拟效果更加逼真。我们使用收集的的五百张真实携带夜间干净光源的图像作为原图。用上面的真实光源模拟辉光生成。我们采用了一种基于动态阈值的光源检测方法，通过设定不同的亮度阈值，精确地检测图像中的光源区域。为了获取到完整的光源区域，应用形态学操作对生成的掩膜（mask）进行处理，以确保光源形状的完整性和连续性。随后，我们对检测到的光源区域进行颜色特征提取。研究表明，相较于光源周围的光晕区域，光源核心区域的颜色饱和度通常更低。基于这一统计规律，我们设计了一种颜色饱和度自适应调整算法，以确保最终生成的光晕颜色更贴近真实场景。为提高数据集的多样性，我们从每幅图像中检测出的多个符合条件的光源中随机选择若干个用于光晕模拟。这种随机化策略有效增加了数据样本的多样性，进一步增强了模型的泛化能力。\n",
        "细节优化：在实现整体光晕与光条（streak）模拟效果的基础上，我们进一步引入了多种细节优化，使生成结果更加贴近真实光晕特性。具体而言，我们在光晕中添加了径向的丝状噪声，从而增强了光晕的光线细节表现，并通过结合细微光线的叠加效果，成功模拟了真实辉光中的闪烁（shimmer）现象。此外，为模拟真实光晕中普遍存在的色散效应，我们设计了色散模拟机制，特别关注边缘区域更为显著的色散特性，从而更好地还原真实场景。\n",
        "为了提高生成光晕的颜色表现力，我们引入了随机的小幅度颜色调整，增强辉光效果的真实性和多样性。\n",
        "针对辉光可能遮挡光源的问题，我们基于光源检测得到的掩膜（mask）对辉光在光源处的亮度进行了抑制。这种处理方式不仅符合真实场景中辉光不会明显遮挡光源的现象，还使得模型在学习辉光去除任务时能够更多地保留光源区域的信息，从而有效缓解光源处理后可能出现的过暗和细节丢失问题。我们使用收集的的五百张真实携带夜间干净光源的图像作为原图。用上面的真实光源模拟辉光生成。我们采用了一种基于动态阈值的光源检测方法，通过设定不同的亮度阈值，精确地检测图像中的光源区域。为了获取到完整的光源区域，应用形态学操作对生成的掩膜（mask）进行处理，以确保光源形状的完整性和连续性。随后，我们对检测到的光源区域进行颜色特征提取。研究表明，相较于光源周围的光晕区域，光源核心区域的颜色饱和度通常更低。基于这一统计规律，我们设计了一种颜色饱和度自适应调整算法，以确保最终生成的光晕颜色更贴近真实场景。为提高数据集的多样性，我们从每幅图像中检测出的多个符合条件的光源中随机选择若干个用于光晕模拟。这种随机化策略有效增加了数据样本的多样性，进一步增强了模型的泛化能力。\n",
        "细节优化：在实现整体光晕与光条（streak）模拟效果的基础上，我们进一步引入了多种细节优化，使生成结果更加贴近真实光晕特性。具体而言，我们在光晕中添加了径向的丝状噪声，从而增强了光晕的光线细节表现，并通过结合细微光线的叠加效果，成功模拟了真实辉光中的闪烁（shimmer）现象。此外，为模拟真实光晕中普遍存在的色散效应，我们设计了色散模拟机制，特别关注边缘区域更为显著的色散特性，从而更好地还原真实场景。\n",
        "为了提高生成光晕的颜色表现力，我们引入了随机的小幅度颜色调整，增强辉光效果的真实性和多样性。\n",
        "辉光与其他类型的图像破坏具有独特的局部特征，因此，模型在学习去除辉光时，如何将注意力集中在辉光区域，对于提升修复效果至关重要。许多先进的图像修复方法（例如Flare7K++）采用了基于Transformer的注意力机制，旨在通过捕捉图像的全局信息来改进修复效果。然而，我们在实验中发现，这种全局注意力机制未能有效地引导模型将注意力集中在辉光区域。辉光区域的特征通常具有较高的局部集中性，而全局注意力机制却倾向于平滑处理图像的整体结构，难以针对局部细节进行精准的处理。\n",
        "在此基础上，我们设计了一个全新的图像修复架构——SharedEncoder-mask2GlowNet。该方法旨在解决传统图像修复模型在处理局部破损时缺乏对破损区域精确关注的问题。该方法的核心思想是通过简单任务的学习来引导复杂任务。我们采用了先对辉光图像进行阈值分割，学习辉光区域的mask，由mask各层特征图指导辉光去除任务。\n",
        "为此，我们在经典的编码器-解码器结构基础上，提出了一种创新性的网络设计，旨在通过共享编码器和双路径解码器的设计。我们保留了常规Transformer架构中的编码器部分，并对解码器进行了创新性的设计，提出了两个并行链路以应对不同的任务需求。其中，第一个链路为主路径，专注于学习辉光去除的过程，旨在通过逐步恢复图像中的结构信息，从而实现辉光的有效去除。第二个链路为辅助路径，专门用于学习辉光区域的掩码生成，目的是准确地识别和定位图像中的辉光区域，为主路径提供有价值的位置信息。\n",
        "在图像处理流程中，信息流通过编码器后，首先进入辅助路径，该路径的任务是学习辉光区域的掩码。为了更好地提升信息流的表达能力，我们设计了一个特征激活增强模块。通过该模块，辅助路径中提取到的特征图会被传递至主路径进行进一步处理。我们对传统的跳跃连接（Skip Connections）进行了创新性改进，以便更加高效地融合辅助路径和主路径之间的信息。具体来说，辅助路径生成的掩码特征图包含了辉光区域的位置信息，并且该掩码图与主路径每一层的特征图保持相同的尺寸，从而确保位置信息可以精确地映射到主路径的特征图上。在信息流的传递过程中，主路径的每一层特征图、辅助路径的特征图以及跳跃连接的特征图都作为输入，传递至我们设计的特征激活增强模块。通过这一模块，掩码特征图能够帮助主路径特征图和跳跃连接的特征图实现选择性的信息融合和增强。经过融合后的信息流将继续作为输出传递至主路径下一层的解码器模块。这一过程会在每一层依次进行，确保信息能够在网络中流动并充分利用不同路径的信息，实现了由学习mask这个简单任务做引导，提供辉光区域的信息和有关特征来强化主路径去辉光的学习。这种创新的设计有效提升了信息流的表达能力，尤其是在处理复杂的辉光区域时，能够更好地捕捉和增强细节。\n",
        "此外，在辅助路径的设计中，我们仅采用较少的注意力头（Attention Heads），以进一步降低计算开销。结合与编码器部分的共享结构，这种设计有效地控制了整体模型参数的增长，从而在保证掩码生成准确度的同时，实现了更高的计算效率与资源利用率。\n",
        "针对辉光可能遮挡光源的问题，我们基于光源检测得到的掩膜（mask）对辉光在光源处的亮度进行了抑制。这种处理方式不仅符合真实场景中辉光不会明显遮挡光源的现象，还使得模型在学习辉光去除任务时能够更多地保留光源区域的信息，从而有效缓解光源处理后可能出现的过暗和细节丢失问题。\n",
        "我们提出了一种特征融合增强模块，其核心思想是利用辅助路径（auxiliary path）学习得到的掩膜（mask）特征图，引导主路径（main path）与跳跃连接（skip connection）的特征融合过程。\n",
        "掩膜特征图仅表征了特定区域与背景之间的粗粒度区分，缺乏对空间位置信息以及通道重要性的有效建模。为了让其在融合过程提供足够的引导信息，我们设计了一个掩膜特征图的增强模块。首先，我们对掩膜特征图进行位置编码，采用多频傅里叶特征嵌入的策略，区别于传统的单一sin(x)/cos(x)或者Transformer中固定频率的位置编码方式，我们的模块显式地采用指数增长的频率序列（如2^k）对二维归一化坐标进行多尺度的正余弦调制。这种多尺度的傅里叶特征嵌入能够有效捕捉从高频到低频的周期性空间结构特征，实现更全面、连续的空间位置编码，引入空间位置感知能力。接着我们引入一个线性映射层，将高维的傅里叶嵌入映射到指定的维度，作为位置编码后的特征，将其与原始掩膜特征图进行拼接，这样的操做增强了mask特征的空间特征表达。最后采用1×1卷积层代替全连接层，加强对于复杂非线性关系的表达能力，生成通道注意与原始掩膜特征图进行逐通道相乘，从而实现特征图通道维度上的动态调整，\n",
        "\n",
        "在特征引导融合过程中，我们首先对输入特征进行了卷积预处理，以提取局部空间特征表示。随后，通过逐元素乘法计算输入特征图（记为x和z）与增强后的掩膜特征图y之间的空间关联性，再通过Sigmoid函数生成范围为[0,1]的空间注意力图。通过这种掩膜引导的注意力机制，我们显式地强调了特征图x和z中与掩膜特征图y高度相关的空间区域。最终，经注意力加权增强后的特征图x与z在进行拼接，从而进一步丰富融合后的特征表示，以提升网络对目标区域的辨别能力。\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "r-_C41fqe6nE"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_extractor = DocumentChunkExtractor()\n",
        "txt = chunk_extractor.extract_important_chunks(doc_text=content, max_chunks=10,chunk_size=256)\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381,
          "referenced_widgets": [
            "636e6ef4e88d439c8c25a6952fb0405f",
            "24d30918af364a4c81ef041ea2864219",
            "edece0c8172347b2bc9610b45a642180",
            "c664d8d516b14c9680ccce821e681b68",
            "45c071ada7df48a8b0323b3b2a5cf92b",
            "2337a472517a4300a9d34541ed141f46",
            "21e4148b864b4f5180c94cc860018d61",
            "239177015db74b489264a24670c0c9b4",
            "d2e0a733fce04a179eb0ea877700fd75",
            "02fe3e5894a3458a8f4f273047c18995",
            "8e88f17253a747deaedb1ed138d2b46b",
            "684869f67bfd41afa5787942f0dceeda",
            "42d20c31729944e3a02ec433f87683f4",
            "e7ef1e88ad8645459e657af02f97a701",
            "fa9ad47960ad4888bba9c740c2cbcdbf",
            "dd475438474f486a8ac32238f040d375",
            "4cbb825ae4764de69d4695d425097ab9",
            "c0f9e6f7f5b345e2b16954f79ae3d5bf",
            "39260c9068644f62be5de51f43bb9682",
            "8653954388454ed58dc6f2c49a7d7c3e",
            "a35f0400c7e54684a77309e4909bbc85",
            "31566f2a0c5d4f5c8788f783d5b1a29b",
            "bc4bbfff125b47709fa7bc109cb8dd71",
            "319ef3e1e9984897b813f46a0b2e2b27",
            "ba1235022bae4ebaa1e49e2fa4a33fd4",
            "78132e0663474396aa2edb523953c4b8",
            "2f53926e35954c77967f27f11bee3488",
            "637812927d02424593e0920f7f5569e0",
            "33fc3f3abdf842c0941ac1ed22d2bb4a",
            "10c3d717fadb49e5adea739b07392735",
            "a6a7c43f67d94b37a558610db59db431",
            "a8bc124f811b44caa689f4128ac8d797",
            "3a501de355e64f1389ccd69bc985c55e",
            "ecfd819ceadb4caeb8be247e025183f1",
            "d2d64f7898f146299e4ac3c7f9ce9f7c",
            "0c015bfac7174c31ab9689f3f916bcf9",
            "27d0cfa2b2fe44e989736522ac6ef6ed",
            "d552f313c7fb426d8c36c6ed256762b9",
            "4e1c83d6e320421e8269aeaaa7f2c8c0",
            "cf79c016a5af48fda6cd237a47e8bed1",
            "e2544d2ebc3e4f3e8e08fd07a3674036",
            "6f75c0ef032b4078bbe13681274c053d",
            "e4f148252fae4f3ba7358f1048832e69",
            "d61ea38399714409a23534041e87316b",
            "d417ef9f784f4015b99154fb92c23174",
            "a72dc5ded79849cdadfadff9d1198311",
            "053eb69f497644b5ae81f2e5e4ef3622",
            "6364fc92098d459e8b08469ed157881b",
            "857fd486a71f470980cbe9de8a8b6a5c",
            "ffd9623fa396495f9c30495be4adcc6a",
            "13a66848a8df4651ba668974caf883ed",
            "d21ea6ed69764920a50384b7e5753736",
            "8633b5e2b74e42c498d011c96a3fea51",
            "990c10bc04974b57a0203d26afb10f48",
            "d952073d457e40bda5dc008f2ff29d16"
          ]
        },
        "id": "uM2Kh31Aei3F",
        "outputId": "d16e7308-9ea0-4d4b-af15-f58a2c8a4d95"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "636e6ef4e88d439c8c25a6952fb0405f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "684869f67bfd41afa5787942f0dceeda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/149k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc4bbfff125b47709fa7bc109cb8dd71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/434M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecfd819ceadb4caeb8be247e025183f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.56M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d417ef9f784f4015b99154fb92c23174"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['随着人类社会夜间活动的增加，各类光源在夜间环境中的广泛应用，造成的问题便是对夜间图像造成了新的干扰，耀斑。这一现象的原因源于镜头光学系统的物理结构特性。镜头通常由多个透镜元件组成，光线在穿过这些透镜时，尽管大部分能够直接透过，但仍有部分光线在透镜表面发生反射。这些反射光在镜头内部多次反射或散射后，最终到达图像传感器，形成光斑或辉光，尤其是当透镜元件数量增加时，这种内部反射和扩散效应会更加显著，从而导致更为明显的眩光现象。尽管现代光学镜头设计已通过多种技术手段在一定程度上缓解了辉光的影响，然而在实际拍摄场景中，镜头表面的灰尘、油渍、雨点、微小划痕，以及空气中的水雾或悬浮颗粒等，都会显著增强光线在镜头表面的散射与反射，进而加剧辉光现象的出现。\\n特别是对于长期暴露在户外环境中的车载摄像头和监控摄像头，以及在日常使用中频繁受到磨损的手机摄像头，外界环境的影响往往难以避免。随着车载系统和监控设备广泛应用于自动驾驶和智能安防等领域，如何在成像后处理环节有效地抑制辉光现象，成为解决这一问题的更为实际和具有针对性的方法。\\n大多数夜间图像增强算法以提高图像', '暗部区域的细节为核心目标。这些方法即使提升了图像的亮度，但经过我们的测试这种亮度提升会造成耀斑现象的加剧，导致更严重的图像破坏，在如今复杂的夜间图像场景中，去除耀斑的任务更加重要。\\n目前去耀斑任务的难点一直在于缺少相关数据集，采集配对的耀斑图像和无耀斑图像是一件非常困难的事情，至今仍然没有真实收集的能够达到训练规模的数据集。一些人也发现了这个问题，whu等人，sun等人基于太阳光的特点生成了耀斑模拟，然而这些方法在与夜间复杂情况下的耀斑状况差别较大，对真实耀斑的处理效果并不好。Yuekun Da 等人先后提出了Flare7K和Flare7K++数据集，这是第一个针对夜间辉光生成的数据集，采用了对辉光完全手工合成，再叠加到真实图像上的想法，是目前与真实耀斑效较为接近的一种方法。然而其手工合成的辉光叠加到图像上往往效果不够自然，其割裂感往往成为一种较为学习的特征。这也就让其训练出的模型在真实图像中，面对柔和的夜间辉光时，往往起不到效果。\\n我们提出了一种创新性的模拟耀斑的方法，让耀斑和真实世界中一样，', '由光源而发，先确定图像中的真实光源，在基于光源进行辉光模拟，而不是预先设定好耀斑的形式。耀斑的特性很大程度上应该由光源本身决定，于是我们首先对图像中光源的特性进行提取，光源的形状，亮度，颜色等。由这些因素限定耀斑的特性。这样生成的耀斑自然程度会比把耀斑直接添加到背景图像上高很多，更加符真实耀斑图像的特征学习难度。为了保证模拟的耀斑和真实的耀斑有着相同的物理规律，特性。我们采集了大概三百张的真实耀斑图像总结出了真实的可进行仿真的规律。为了充分模拟夜间耀斑的复杂多样性，基于这些规律我们不限定具体的值，只给定合理的范围，在合理的范围内随机设定各类参数。同时我们设计的，查找图像中可能光源的函数，允许一张图像中同时存在多个候选光源。我们会为每个光源分别生成辉光，这些操做可以保证我们数据集可以根据一定数量的夜间背景图生成大量的配对图像。\\n辉光具有较强的局部性，我们发现Flare7K的有监督方法中部分图像处理效果不好的原因在于模型无法正确感知到辉光的位置。基于此我们想通过一个简单的阈值分割任务来引导模型学习辉光的位置，', '表现为线状的耀斑或条纹（streaks），Streak（条形耀斑）是辉光的一种重要组成部分，具有与光晕相似的径向亮度衰减特性，但同时表现出显著的环向分布特性和亮度边界。我们同样对收集到的辉光图像进行定量分析，总结了Streak的以下特性：环向分布特性：\\nStreak通常出现在特定角度，呈现条状分布，可分为以下两种典型形态：宽条状： 宽度较大，亮度高于周围光晕，透明度较高，衰减较为明显。细线状： 宽度较小，亮度非常高，透明度较低，衰减不显著。周围光晕的“缺口效应”：Streak的出现通常伴随着两侧光晕亮度的下降，形成类似“缺口”的区域。\\n基于上述特性我们分别设计模拟两种streaks的形态，每次随机角度生成，引入径向噪声：为增强光线的丝状的真实感，我们在径向方向上加入丝状噪声。在Streak两侧光晕区域内引入亮度抑制函数，通过降低光晕亮度实现“缺口效应”的模拟。抑制程度与Streak的宽度和亮度相关联，以确保模拟效果更加逼真。我们使用收集的的五百张真实携带夜间干净光源的图像作为原图。用上面的真实', '光源模拟辉光生成。我们采用了一种基于动态阈值的光源检测方法，通过设定不同的亮度阈值，精确地检测图像中的光源区域。为了获取到完整的光源区域，应用形态学操作对生成的掩膜（mask）进行处理，以确保光源形状的完整性和连续性。随后，我们对检测到的光源区域进行颜色特征提取。研究表明，相较于光源周围的光晕区域，光源核心区域的颜色饱和度通常更低。基于这一统计规律，我们设计了一种颜色饱和度自适应调整算法，以确保最终生成的光晕颜色更贴近真实场景。为提高数据集的多样性，我们从每幅图像中检测出的多个符合条件的光源中随机选择若干个用于光晕模拟。这种随机化策略有效增加了数据样本的多样性，进一步增强了模型的泛化能力。\\n细节优化：在实现整体光晕与光条（streak）模拟效果的基础上，我们进一步引入了多种细节优化，使生成结果更加贴近真实光晕特性。具体而言，我们在光晕中添加了径向的丝状噪声，从而增强了光晕的光线细节表现，并通过结合细微光线的叠加效果，成功模拟了真实辉光中的闪烁（shimmer）现象。此外，为模拟真实光晕中普遍存在的色散效应，我们设计了色', '首先进入辅助路径，该路径的任务是学习辉光区域的掩码。为了更好地提升信息流的表达能力，我们设计了一个特征激活增强模块。通过该模块，辅助路径中提取到的特征图会被传递至主路径进行进一步处理。我们对传统的跳跃连接（Skip Connections）进行了创新性改进，以便更加高效地融合辅助路径和主路径之间的信息。具体来说，辅助路径生成的掩码特征图包含了辉光区域的位置信息，并且该掩码图与主路径每一层的特征图保持相同的尺寸，从而确保位置信息可以精确地映射到主路径的特征图上。在信息流的传递过程中，主路径的每一层特征图、辅助路径的特征图以及跳跃连接的特征图都作为输入，传递至我们设计的特征激活增强模块。通过这一模块，掩码特征图能够帮助主路径特征图和跳跃连接的特征图实现选择性的信息融合和增强。经过融合后的信息流将继续作为输出传递至主路径下一层的解码器模块。这一过程会在每一层依次进行，确保信息能够在网络中流动并充分利用不同路径的信息，实现了由学习mask这个简单任务做引导，提供辉光区域的信息和有关特征来强化主路径去辉光的学习。这种创新的设计有效提升了信息流的表达能力', '多频傅里叶特征嵌入的策略，区别于传统的单一sin(x)/cos(x)或者Transformer中固定频率的位置编码方式，我们的模块显式地采用指数增长的频率序列（如2^k）对二维归一化坐标进行多尺度的正余弦调制。这种多尺度的傅里叶特征嵌入能够有效捕捉从高频到低频的周期性空间结构特征，实现更全面、连续的空间位置编码，引入空间位置感知能力。接着我们引入一个线性映射层，将高维的傅里叶嵌入映射到指定的维度，作为位置编码后的特征，将其与原始掩膜特征图进行拼接，这样的操做增强了mask特征的空间特征表达。最后采用1×1卷积层代替全连接层，加强对于复杂非线性关系的表达能力，生成通道注意与原始掩膜特征图进行逐通道相乘，从而实现特征图通道维度上的动态调整，\\n\\n在特征引导融合过程中，我们首先对输入特征进行了卷积预处理，以提取局部空间特征表示。随后，通过逐元素乘法计算输入特征图（记为x和z）与增强后的掩膜特征图y之间的空间关联性，再通过Sigmoid函数生成范围为[0,1]的空间注意力图。通过这种掩膜引导的注意力', '机制，我们显式地强调了特征图x和z中与掩膜特征图y高度相关的空间区域。最终，经注意力加权增强后的特征图x与z在进行拼接，从而进一步丰富融合后的特征表示，以提升网络对目标区域的辨别能力。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@structured(llm_client=DEFAULT_TEXT_LLM,model=\"deepseek-chat\")\n",
        "def doc_sketch(txt: str,max_chunks, chunk_size) -> str:\n",
        "\n",
        "    chunk_extractor = DocumentChunkExtractor()\n",
        "    txt = chunk_extractor.extract_important_chunks(doc_text=content, max_chunks=10,chunk_size=256)\n",
        "\n",
        "\n",
        "    return f\"\"\"\n",
        "    请根据以下提供的文档，生成一个合适的标题和一段 400-500 字的摘要。\n",
        "    标题应简洁明了，能够准确概括文档的核心内容。摘要应完整呈现文档的主要论点、关键信息和结论，要确保一定提到文档中的每个片段，适用于后续检索和快速理解原文内容。\n",
        "    请按照 JSON 格式输出，确保格式规范且可解析，\n",
        "    格式如下：{{\n",
        "      \"title\":\"生成的标题\",\n",
        "      \"language\":文章语言：用英文简写（en，zh_CN等，根据文章语言判断）\n",
        "      \"summary\":生成的摘要}}\n",
        "    请确保 title 反映文档核心内容，summary 精炼但包括所有段落的内容。\n",
        "    输入内容如下：{txt}\n",
        "    现在，不要给出任何解释性文本，请直接输出:\n",
        "    \"\"\"\n",
        "result = doc_sketch(txt=content,max_chunks=10,chunk_size=256)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhUxM8e_gnH0",
        "outputId": "4394edb0-6a20-4a3f-8566-4af18335fbcf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': '夜间图像辉光现象的模拟与去除方法研究', 'language': 'zh_CN', 'summary': '本文探讨了夜间图像中辉光现象的成因及其去除方法。辉光现象主要由镜头光学系统的物理结构特性引起，光线在透镜表面反射和散射后形成光斑或辉光，尤其在透镜元件数量增加时更为明显。尽管现代光学设计已部分缓解此问题，但镜头表面的灰尘、油渍等环境因素仍会加剧辉光现象。车载摄像头、监控摄像头和手机摄像头等长期暴露在户外或频繁磨损的设备尤其受到影响。\\n\\n目前，夜间图像增强算法虽能提升暗部细节，但会加剧耀斑现象。去耀斑任务的难点在于缺乏真实配对的数据集。现有方法如Flare7K和Flare7K++数据集通过手工合成辉光并叠加到真实图像上，但效果不够自然，导致模型在真实场景中表现不佳。\\n\\n本文提出了一种创新性的耀斑模拟方法，通过提取图像中光源的形状、亮度、颜色等特性来生成更自然的耀斑。该方法基于300张真实耀斑图像总结的规律，在合理范围内随机设定参数，并允许一张图像中存在多个候选光源，从而生成大量配对图像。此外，通过阈值分割任务引导模型学习辉光位置，提升处理效果。\\n\\n针对辉光的重要组成部分——条形耀斑（Streak），本文分析了其环向分布特性和亮度边界，设计了两种典型形态的模拟方法，并引入径向噪声和亮度抑制函数以增强真实感。研究还采用动态阈值的光源检测方法和颜色饱和度自适应调整算法，确保光晕颜色贴近真实场景。\\n\\n在模型设计上，本文提出了一种辅助路径学习辉光区域掩码的方法，通过特征激活增强模块和改进的跳跃连接，实现信息高效融合。此外，引入多频傅里叶特征嵌入策略，增强空间位置感知能力，并通过掩膜引导的注意力机制提升网络对目标区域的辨别能力。这些创新设计显著提升了辉光去除的效果。'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from http import HTTPStatus\n",
        "from dashscope.audio.asr import Transcription\n",
        "import json\n",
        "\n",
        "\n",
        "import dashscope\n",
        "dashscope.api_key = \"sk-7f9a260343a54674b720a2d5fa772a5d\"\n",
        "\n",
        "transcribe_response = Transcription.async_call(\n",
        "    model='paraformer-v2',\n",
        "    file_urls=['https://uy.wzznft.com/i/2025/04/15/p3cj3l.mp3',\n",
        "          'https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_male2.wav'],\n",
        "    diarization_enabled=True,\n",
        "    language_hints=['zh', 'en']  # “language_hints”只支持paraformer-v2模型\n",
        ")\n",
        "\n",
        "while True:\n",
        "    if transcribe_response.output.task_status == 'SUCCEEDED' or transcribe_response.output.task_status == 'FAILED':\n",
        "        break\n",
        "    transcribe_response = Transcription.fetch(task=transcribe_response.output.task_id)\n",
        "\n",
        "if transcribe_response.status_code == HTTPStatus.OK:\n",
        "    print(json.dumps(transcribe_response.output, indent=4, ensure_ascii=False))\n",
        "    print('transcription done!')"
      ],
      "metadata": {
        "id": "-TggVZxHpJ_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7215b601-152d-44be-d85d-23415a785bf1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"task_id\": \"c2ad5c4d-fd89-487a-bb6b-bc18d3467985\",\n",
            "    \"task_status\": \"SUCCEEDED\",\n",
            "    \"submit_time\": \"2025-04-15 15:41:20.472\",\n",
            "    \"scheduled_time\": \"2025-04-15 15:41:20.491\",\n",
            "    \"end_time\": \"2025-04-15 15:43:55.793\",\n",
            "    \"results\": [\n",
            "        {\n",
            "            \"file_url\": \"https://uy.wzznft.com/i/2025/04/15/p3cj3l.mp3\",\n",
            "            \"transcription_url\": \"https://dashscope-result-bj.oss-cn-beijing.aliyuncs.com/prod/paraformer-v2/20250415/15%3A43/649048e8-667c-4934-9d34-2a73cfb52f53-1.json?Expires=1744789435&OSSAccessKeyId=LTAI5tQZd8AEcZX6KZV4G8qL&Signature=fnspp34i5XEVgnciYp%2FmGhx7BiI%3D\",\n",
            "            \"subtask_status\": \"SUCCEEDED\"\n",
            "        },\n",
            "        {\n",
            "            \"file_url\": \"https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_male2.wav\",\n",
            "            \"transcription_url\": \"https://dashscope-result-bj.oss-cn-beijing.aliyuncs.com/prod/paraformer-v2/20250415/15%3A41/a0bb646f-54c8-4b9b-8d07-a986b0d9b787-1.json?Expires=1744789435&OSSAccessKeyId=LTAI5tQZd8AEcZX6KZV4G8qL&Signature=ldfoTVDZZtVWb%2FVdiSWJpjyccnk%3D\",\n",
            "            \"subtask_status\": \"SUCCEEDED\"\n",
            "        }\n",
            "    ],\n",
            "    \"task_metrics\": {\n",
            "        \"TOTAL\": 2,\n",
            "        \"SUCCEEDED\": 2,\n",
            "        \"FAILED\": 0\n",
            "    }\n",
            "}\n",
            "transcription done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from http import HTTPStatus\n",
        "from dashscope.audio.asr import Transcription\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "# 设置API密钥\n",
        "import dashscope\n",
        "dashscope.api_key = \"sk-7f9a260343a54674b720a2d5fa772a5d\"\n",
        "\n",
        "def extract_sentences_from_transcription(transcription_content):\n",
        "    \"\"\"\n",
        "    Extract the sentences content from transcription JSON without word details\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract the sentences from the transcript\n",
        "        sentences = transcription_content[\"transcripts\"][0][\"sentences\"]\n",
        "\n",
        "        # Remove the 'words' field from each sentence\n",
        "        for sentence in sentences:\n",
        "            if \"words\" in sentence:\n",
        "                del sentence[\"words\"]\n",
        "\n",
        "        return sentences\n",
        "    except (KeyError, IndexError) as e:\n",
        "        print(f\"Error extracting sentences: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def get_transcription_content(transcription_result):\n",
        "    \"\"\"\n",
        "    Extract and download transcription content from all URLs in the result\n",
        "    \"\"\"\n",
        "    all_sentences = {}\n",
        "\n",
        "    # Extract all result items that have succeeded\n",
        "    successful_items = [item for item in transcription_result[\"results\"]\n",
        "                        if item[\"subtask_status\"] == \"SUCCEEDED\"]\n",
        "\n",
        "    for item in successful_items:\n",
        "        file_url = item[\"file_url\"]\n",
        "        transcription_url = item[\"transcription_url\"]\n",
        "\n",
        "        try:\n",
        "            response = requests.get(transcription_url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Parse the JSON content\n",
        "            transcription_content = response.json()\n",
        "\n",
        "            # Extract sentences content without words\n",
        "            sentences_content = extract_sentences_from_transcription(transcription_content)\n",
        "            if sentences_content:\n",
        "                all_sentences[file_url] = sentences_content\n",
        "\n",
        "            print(f\"Successfully retrieved transcription for: {file_url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error retrieving transcription for {file_url}: {str(e)}\")\n",
        "\n",
        "    return all_sentences\n",
        "\n",
        "def transcribe_audio(file_urls, language_hints=['zh', 'en']):\n",
        "    \"\"\"\n",
        "    Transcribe audio files and return the extracted sentences content without word details\n",
        "\n",
        "    Args:\n",
        "        file_urls: List of URLs of audio files to transcribe\n",
        "        language_hints: List of language hints for the transcription model\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping file URLs to their transcribed sentences content\n",
        "    \"\"\"\n",
        "    dashscope.api_key = \"sk-7f9a260343a54674b720a2d5fa772a5d\"\n",
        "\n",
        "    # 发起异步转写请求\n",
        "    transcribe_response = Transcription.async_call(\n",
        "        model='paraformer-v2',\n",
        "        file_urls=file_urls,\n",
        "        language_hints=language_hints,\n",
        "        diarization_enabled=True\n",
        "    )\n",
        "\n",
        "    # 循环检查任务状态，直到完成或失败\n",
        "    while True:\n",
        "        if transcribe_response.output.task_status == 'SUCCEEDED' or transcribe_response.output.task_status == 'FAILED':\n",
        "            break\n",
        "        transcribe_response = Transcription.fetch(task=transcribe_response.output.task_id)\n",
        "\n",
        "    # 检查转写是否成功\n",
        "    if transcribe_response.status_code == HTTPStatus.OK:\n",
        "        # 获取句子内容（无单词详情）\n",
        "        sentences_contents = get_transcription_content(transcribe_response.output)\n",
        "        return sentences_contents\n",
        "    else:\n",
        "        print(f\"Transcription failed with status code: {transcribe_response.status_code}\")\n",
        "        return {}\n",
        "\n",
        "# 示例使用方式\n",
        "if __name__ == \"__main__\":\n",
        "    file_urls = [\n",
        "        'https://geilien.cn/gaokao/2024/mp3/quanguoyijuan.mp3',\n",
        "        'https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_male2.wav'\n",
        "    ]\n",
        "    start_time = time.time()\n",
        "    results = transcribe_audio(file_urls)\n",
        "    end_time = time.time()\n",
        "    time_long=end_time - start_time\n",
        "    print(time_long)\n",
        "    #print(results)\n",
        "    # 显示结果\n",
        "\n",
        "    # for file_url, sentences in results.items():\n",
        "    #     file_name = file_url.split('/')[-1]\n",
        "    #     print(f\"\\n{file_name} sentences:\")\n",
        "    #     print(json.dumps(sentences, indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mQ8_4VBDklv",
        "outputId": "66d9e580-fd13-4a26-e880-3f4f5e0d799f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully retrieved transcription for: https://geilien.cn/gaokao/2024/mp3/quanguoyijuan.mp3\n",
            "Successfully retrieved transcription for: https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_male2.wav\n",
            "178.95487546920776\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_sentences_by_file(results):\n",
        "    \"\"\"\n",
        "    Convert sentences results to formatted strings grouped by file.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary mapping file URLs to sentences information\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping file names to their formatted sentence strings\n",
        "    \"\"\"\n",
        "    formatted_strings_by_file = {}\n",
        "\n",
        "    for file_url, sentences in results.items():\n",
        "        file_name = file_url.split('/')[-1]\n",
        "        sentences_strings = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            begin_time_sec = sentence['begin_time'] / 1000  # Convert ms to seconds\n",
        "            end_time_sec = sentence['end_time'] / 1000      # Convert ms to seconds\n",
        "\n",
        "            sentence_str = (\n",
        "                f\"Sentence ID: {sentence['sentence_id']}, \"\n",
        "                f\"Time: [{begin_time_sec:.2f}s - {end_time_sec:.2f}s], \"\n",
        "\n",
        "                f\"Text: \\\"{sentence['text']}\\\"\"\n",
        "            )\n",
        "\n",
        "            sentences_strings.append(sentence_str)\n",
        "\n",
        "        # Join all sentences for this file with newlines\n",
        "        formatted_strings_by_file[file_name] = \"\\n\".join(sentences_strings)\n",
        "\n",
        "    return formatted_strings_by_file\n",
        "\n",
        "# 示例用法\n",
        "if __name__ == \"__main__\":\n",
        "    # 您的结果示例\n",
        "    # results = {\n",
        "    #     'https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_male2.wav': [\n",
        "    #         {'begin_time': 0, 'end_time': 4720, 'text': 'Hello world, 这里是阿里巴巴语音实验室。', 'sentence_id': 1, 'speaker_id': 0}\n",
        "    #     ],\n",
        "    #     'https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_female2.wav': [\n",
        "    #         {'begin_time': 100, 'end_time': 3820, 'text': 'Hello word, 这里是阿里巴巴语音实验室。', 'sentence_id': 1, 'speaker_id': 0},\n",
        "    #         {'begin_time': 4000, 'end_time': 5500, 'text': '欢迎使用我们的服务。', 'sentence_id': 2, 'speaker_id': 0}\n",
        "    #     ]\n",
        "    # }\n",
        "    file_urls = [\n",
        "        'http://sound2.yywz123.com/english96ad/lesson/tinglimeirlian1/sound/14937facefdcebdba.mp3',\n",
        "        'https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_male2.wav'\n",
        "    ]\n",
        "\n",
        "    results = transcribe_audio(file_urls)\n",
        "\n",
        "    # 转换为按文件分组的字符串\n",
        "    formatted_outputs = format_sentences_by_file(results)\n",
        "\n",
        "    # 打印结果\n",
        "    for file_name, formatted_text in formatted_outputs.items():\n",
        "        print(f\"=== {file_name} ===\")\n",
        "        print(formatted_text)\n",
        "        print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "zdr0qAweNPTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16a30b8-b7bd-4d38-e980-8aa7dac5a141"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully retrieved transcription for: http://sound2.yywz123.com/english96ad/lesson/tinglimeirlian1/sound/14937facefdcebdba.mp3\n",
            "Successfully retrieved transcription for: https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_male2.wav\n",
            "=== 14937facefdcebdba.mp3 ===\n",
            "Sentence ID: 1, Time: [2.80s - 6.66s], Text: \"Here's a picture we couldn't see star last night. \"\n",
            "Sentence ID: 2, Time: [6.66s - 7.76s], Text: \"Yes, was a pity. \"\n",
            "Sentence ID: 3, Time: [7.76s - 9.96s], Text: \"I here's was one of us here. \"\n",
            "Sentence ID: 4, Time: [9.96s - 19.96s], Text: \"If everyone says it's a really great film, let's see the things you like that horror films and filllers and science fiction. \"\n",
            "Sentence ID: 5, Time: [19.96s - 23.51s], Text: \"That's why I want to see Star Wars come back. \"\n",
            "Sentence ID: 6, Time: [23.51s - 26.09s], Text: \"You what kind of films do you like? \"\n",
            "Sentence ID: 7, Time: [26.09s - 28.34s], Text: \"Historical films and comedy. \"\n",
            "Sentence ID: 8, Time: [28.34s - 29.96s], Text: \"I see nothing more. \"\n",
            "Sentence ID: 9, Time: [29.96s - 35.12s], Text: \"And riding films or I have you seen the music box more several time? \"\n",
            "\n",
            "\n",
            "=== hello_world_male2.wav ===\n",
            "Sentence ID: 1, Time: [0.00s - 4.72s], Text: \"Hello world, 这里是阿里巴巴语音实验室。\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_sentences_by_file(results):\n",
        "    \"\"\"\n",
        "    Extract and concatenate all sentence texts for each file.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary mapping file URLs to sentences information\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping file names to concatenated text\n",
        "    \"\"\"\n",
        "    text_by_file = {}\n",
        "\n",
        "    for file_url, sentences in results.items():\n",
        "        file_name = file_url.split('/')[-1]\n",
        "\n",
        "        # Extract only the text from each sentence and join them\n",
        "        all_text = \" \".join([sentence['text'] for sentence in sentences])\n",
        "\n",
        "        # Store the combined text for this file\n",
        "        text_by_file[file_name] = all_text\n",
        "\n",
        "    return text_by_file\n",
        "\n",
        "# 示例用法\n",
        "if __name__ == \"__main__\":\n",
        "    file_urls = [\n",
        "        'https://geilien.cn/gaokao/2024/mp3/quanguoyijuan.mp3',\n",
        "        'https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_male2.wav'\n",
        "    ]\n",
        "\n",
        "    results = transcribe_audio(file_urls)\n",
        "\n",
        "    # 提取并合并每个文件的文本内容\n",
        "    text_by_file = format_sentences_by_file(results)\n",
        "\n",
        "    # 打印结果\n",
        "    for file_name, text in text_by_file.items():\n",
        "        print(f\"=== {file_name} ===\")\n",
        "        print(text)\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn-RJHE03IzW",
        "outputId": "1196a3d6-b066-451d-eeba-3947da8acf06"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully retrieved transcription for: https://geilien.cn/gaokao/2024/mp3/quanguoyijuan.mp3\n",
            "Successfully retrieved transcription for: https://dashscope.oss-cn-beijing.aliyuncs.com/samples/audio/paraformer/hello_world_male2.wav\n",
            "=== quanguoyijuan.mp3 ===\n",
            "这是2024年普通高等学校招生全国统一考试英语科听力部分，该部分分为第一、第二、两节。 注意，回答听力部分时，请先将答案标在试卷上。 听力部分结束前，你将有两分钟的时间将你的答案转图到答题卡上。 现在是听力试音时间。 Hello, international friends club.  Can I help you?  Oh, hello, I read about your club in the paper today, and I thought I'd phto find out a bit more.  Yes, certainly.  Well, we're a sort of social club for people from different countries.  It's quite a new club.  We have about 50 members at the moment, but we're growing all the time.  That sounds interesting.  I'm British, actually, and I came to Washington about three months ago.  I'm looking for ways to meet people.  What kind of events do .  you organize?  Well, we have social togeand, sports events, and we also have language ings.  Could you tell me something about the language evenings?  Yes.  every day except Thursday, we have a language evening.  People can come and practice their languages, you know, over a drink or something.  We have different languages on different evenings.  Monday Spanish, Tuesday Italian, Wednesday German and Friday French.  On Thursday, we usually have a meal in a restaurant for anyone who wants to come.  Well.  that sounds great.  I really need to practice my French.  Okay, well, if you can just give me your name and address, I'll send you the form and some more information.  If you join now, you can have .  the first .  month free试音到此结束。 听力考试正式开始。 请看听力部分，第一节第一节听下面5段对话，每段对话后有一个小题，从题中所给的ABC3个选项中选出最佳选项。 听完每段对话后，你都有10秒钟的时间来回答有关小题和阅读下一小题，每段对话仅读一遍。 例如现在你有5秒钟的时间看试卷上的例题。 你将听到以。 下内容，excuse me.  the shirt is it's 9:15.  你将有5秒钟的时间将正确答案标在试卷上。 衬衫的价格为九磅，15便是，所以你选择C项并将其标在试卷上。 现在你有5秒钟的时间阅读第一小题的有关内容。 Thanks for the wonderful weekend.  Kate.  That's okay.  Bob and I are glad you came to see us.  Oh, I have to go in my flight.  Will take off soon.  Do contact me when you're .  in Sydney.  Sure we will.  Paul, listen to the radio.  It's you've stolen my heart, one of the songs played at .  our wedding.  Yeah, how beautiful.  It has been popular for almost two decades.  David, forget about mark.  His aunt is in town, so he can't go .  with us today.  What a pity.  It's the last day of the art show.  How may I help you?  I bought .  a desk and asked for it to be delivered to my .  house this Friday.  Yes, what's the problem?  I need to have it delivered this Saturday.  Next, please.  Oh.  hi.  I missed my 9:00 train to Bedford.  Do I have to buy another ticket?  No.  The next train leaves at a 9:45 at platform eleven.  Thank you.  第一节到此结束。 第二节听下面5段对话或独白。 每段对话或独白后有几个小题，从题中所给的ABC3个选项中选出最佳选项。 听每段对话或读白前，你将有时间阅读各个小题，每小题5秒钟。 听完后，各小题将给出5秒钟的作答时间，每段对话或独白读两遍。 听下面一段对话，回答第六和第7两个小题。 现在你有10秒钟的时间阅读这两个小题。 Honey, have you checked today's weather .  forecast?  Yes, it's cold and wet.  There is a warning for strong winds up to 100 km per hour.  What are we going to do then?  Nothing much.  Just stay indoors.  There is a risk, falling trees and power lines.  right?  And the low temperatures could bring snow to the forest area.  I hope it's over quickly.  Well, it won't get better until late Wednesday anyway.  I have to move the car away .  from the trees.  Yeah, you cannot not be too careful, honey.  Have you checked today's weather .  forecast?  Yes, it's cold and wet.  There is a warning for strong winds up to 100 km per hour.  What are we going to do then?  Nothing much.  Just stay indoors.  There is a risk falling trees and power lines.  right?  And the low temperatures .  could bring snow to the forest area.  I hope it's over quickly.  Well, it won't get better until late Wednesday anyway.  I have to move the car away .  from the trees.  Yeah, you cannot not be too careful.  听下面一段对话，回答第八至第13个小题。 现在你有15秒钟的时间阅读这三个小题。 Hello Dave.  This is Kathy from Sunny California.  Hi Kathy, you finally called.  How was the move all settled in?  Sorry I .  hadn't called sooner, but it's been a busy month.  We're slowly getting things set up in our new home.  Yeah, I understand.  How are Jeff and the children?  Jeff is doing well with his new job.  Tom has made many new friends here and has a lot to do.  Fiona is fine though she misses her grandma.  By the way, thank you for looking in on my mother from time to time.  I call her every week, but it isn't the same as .  seeing her.  No problem.  Betty and I are friends now.  How is the .  weather there?  It's nice and warm and we are able to spend some time every week on the beach with the children.  That's great.  Hello Dave.  Is Kathy .  from Sunny California?  Hi Kathy.  You finally called.  How was the move all settled in?  Sorry I hadn't called sooner.  but it's been a busy month.  We're slowly getting things set up in our new home.  Yeah, I understand.  How are Jeff and the children?  Jeff is doing well with his new job.  Tom has made many new friends here and has a lot to do.  Fiona is fine, though.  She misses her grandma.  By the way, thank you for looking in on my mother from time to time.  I call her every week, but it isn't the .  same as seeing her.  No problem.  Betty and I are friends now.  How is the weather there?  It's nice and warm, and we are able to spend some time every week on the beach .  with the children.  That's great.  听下面一段对话，回答第11至第13 3个小题。 现在您有15秒钟的时间阅读这三个小题。 Jack, how did you get to school when you were in primary school?  I lived close to my school, so I walked every day.  Why?  Well, I remember .  that when we were kids, we often walked, rode a bike or caught the bus to school.  Few of us were dropped off at the school gate .  by our parents.  I see what you mean.  These days, you can see traffic jams around schools that drop off and pick kup times, but it's hard to blame the parents.  They have good reasons for driving their kids to school, mostly to do with safety .  and convenience.  You have a point there, but it could also mean children are missing out on much needed exercise and other life skills.  Some parents are just being overprotective with their children, learning nothing but living in fear of everything.  Studies have found that children who spend more time outside tend to be healthier, better adjusted and better at dealing with stress.  Jack, how did you get to school when you were in primary school?  I lived close to my school, so I walked every day.  Why?  Well.  I remember that when we were kids, we often walked, rode a bike or caught the bus to school.  Few of us were dropped off at the school gate .  by our parents.  I see what you mean.  These days, you can see traffic jams around schools that drop off and pick kup times, but it's hard to blame the parents.  They have good reasons for driving their kids to school, mostly to do with safety .  and convenience.  You have a point there, but it could also mean children are missing out on much needed exercise and other life skills.  Some parents are just being overprotective with their children, learning nothing but living in fear of everything.  Studies have found that children who spend more time outside tend to be healthier, better adjusted and better at dealing with stress.  听下面一段对话，回答第14至第17 4个小题。 现在你有20秒钟的时间阅读这四个小题。 So Marie, your kitchen garden looks excellent.  What made you turn to social media to record your vegetable growing?  Initially, I used the online platform as a diary, something to look back on, giving me a sense of achievement and keeping me motivated and moving forward.  As time went by, other gardeners and like minded people began to follow my progress too.  I know you grow lots of fruit on your land.  Which would you recommend to beginners .  as the best to grow strawberries would be a good choice.  They produce a lot of fruit in their .  first season.  That's cool.  Well, do you have plans to try new or any particular .  crops next year?  Next season I will be adding some pear trees to the fruit area.  I will be adding more herbs, which I can use in the kitchen.  And after a couple of years of failure, I will try .  growing carrots again.  What advice would you offer someone thinking of doing kitchen gardening?  Have a plan of what you want your kitchen garden to look like.  Don't be too discouraged if things don't go according to plan.  Learn from your mistakes and move on.  There's is always next season.  So Marie, your kitchen garden looks excellent.  What made you turn to social media to record your vegetable growing?  Initially, I used the online platform as a diary, something to look back on, giving me a sense of achievement and keeping me motivated and moving forward.  As time went by, other gardeners and like minded people began to follow my progress too.  I know you grow lots of fruit on your land.  Which would you recommend to beginners .  as the best to grow strawberries would be a good choice.  They produce a lot of fruit in their .  first season.  That's cool.  Well, do you have plans to try new or any particular .  crops next year?  Next season, I will be adding some pear trees to the fruit area, I will be adding more herbs, which I can use in the kitchen, and after a couple of years of failure, I will try .  growing carrots again.  What advice would you offer someone thinking of doing kitchen .  gardening?  Have a plan of what you want your kitchen garden to look like.  Don't be too discouraged if things don't go according to plan.  Learn from your mistakes and move on.  There is always next season.  拼下面一段独白，回答第18至第23个小题。 现在你有15秒钟的时间阅读这三个小题。 Welcome to meet the author.  Well, many readers of sports times turn to the last page of their magazine first in order to read Jacob Johnson's weekly article under the title life of Johnson.  The articles, along with his novels and essay collections, have earned Johnson the reputation as one of the funniest humans on the planet.  Johnson began writing about sports as a second year student at the University of\tColorado, covering high school volleyball games for his hometown newspaper.  After graduating in 1981, he moved on to work at the Denver weekly for two years and the Los Angeles post for two more years before landing at sports times.  He has been voted national sports writer of the year eleven times.  So now let's welcome the funny man with serious talent, Jacob Johnson.  Welcome to meet the author.  Well, many readers of sports times turn to the last page of their magazine first in order to read Jacob Johnson's weekly article under the title life of Johnson.  The articles, along with his novels and essay collections, have earned Johnson the reputation as one of the funniest humans on the planet.  Johnson began writing about sports as a second year student at the University of\tColorado, covering high school volleyball games for his hometown newspaper.  After graduating in 1981, he moved on to work at the Denver weekly for two years and the Los Angeles post for two more years before landing at sports times.  He has been voted national sports writer of the year eleven times.  So now let's welcome the funny man with serious talent, Jacob Johnson.  第二节到此结束，现在你有两分钟的时间将试卷上的答案转图到答题卡上。 听力部分到此结束。\n",
            "\n",
            "\n",
            "=== hello_world_male2.wav ===\n",
            "Hello world, 这里是阿里巴巴语音实验室。\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moviepy"
      ],
      "metadata": {
        "id": "fUHcx_gqPfsM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "def video_to_audio(input_video_path, output_audio_path):\n",
        "    \"\"\"\n",
        "    将视频文件转换为音频文件\n",
        "\n",
        "    参数:\n",
        "        input_video_path (str): 输入视频文件的路径\n",
        "        output_audio_path (str): 输出音频文件的路径（建议使用.mp3或.wav扩展名）\n",
        "\n",
        "    返回:\n",
        "        bool: 转换成功返回True，失败返回False\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 加载视频文件\n",
        "        video_clip = VideoFileClip(input_video_path)\n",
        "\n",
        "        # 提取音频\n",
        "        audio_clip = video_clip.audio\n",
        "\n",
        "        # 将音频保存到指定路径\n",
        "        audio_clip.write_audiofile(output_audio_path)\n",
        "\n",
        "        # 关闭资源\n",
        "        audio_clip.close()\n",
        "        video_clip.close()\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"转换过程中出现错误: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    input_path = \"1.mp4\"  # 输入视频路径\n",
        "    output_path = \"1.mp3\"  # 输出音频路径\n",
        "\n",
        "    success = video_to_audio(input_path, output_path)\n",
        "    if success:\n",
        "        print(f\"视频已成功转换为音频并保存到: {output_path}\")\n",
        "    else:\n",
        "        print(\"视频转换失败。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDSn7QmRPcIU",
        "outputId": "9464b2c3-f4de-4479-e60b-2d7b0e16d89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in 1.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "视频已成功转换为音频并保存到: 1.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8nHxigpipJwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text='''Sentence ID: 1, Time: [2.80s - 6.66s], Text: \"Here's a picture we couldn't see star last night. \"\n",
        "Sentence ID: 2, Time: [6.66s - 7.76s], Text: \"Yes, was a pity. \"\n",
        "Sentence ID: 3, Time: [7.76s - 9.96s], Text: \"I here's was one of us here. \"\n",
        "Sentence ID: 4, Time: [9.96s - 19.96s], Text: \"If everyone says it's a really great film, let's see the things you like that horror films and filllers and science fiction. \"\n",
        "Sentence ID: 5, Time: [19.96s - 23.51s], Text: \"That's why I want to see Star Wars come back. \"\n",
        "Sentence ID: 6, Time: [23.51s - 26.09s], Text: \"You what kind of films do you like? \"\n",
        "Sentence ID: 7, Time: [26.09s - 28.34s], Text: \"Historical films and comedy. \"\n",
        "Sentence ID: 8, Time: [28.34s - 29.96s], Text: \"I see nothing more. \"\n",
        "Sentence ID: 9, Time: [29.96s - 35.12s], Text: \"And riding films or I have you seen the music box more several time? \"\n",
        "'''"
      ],
      "metadata": {
        "id": "WN_xzZfnvE3F"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content='''\n",
        "这是2024年普通高等学校招生全国统一考试英语科听力部分，该部分分为第一、第二、两节。 注意，回答听力部分时，请先将答案标在试卷上。 听力部分结束前，你将有两分钟的时间将你的答案转图到答题卡上。 现在是听力试音时间。 Hello, international friends club.  Can I help you?  Oh, hello, I read about your club in the paper today, and I thought I'd phto find out a bit more.  Yes, certainly.  Well, we're a sort of social club for people from different countries.  It's quite a new club.  We have about 50 members at the moment, but we're growing all the time.  That sounds interesting.  I'm British, actually, and I came to Washington about three months ago.  I'm looking for ways to meet people.  What kind of events do .  you organize?  Well, we have social togeand, sports events, and we also have language ings.  Could you tell me something about the language evenings?  Yes.  every day except Thursday, we have a language evening.  People can come and practice their languages, you know, over a drink or something.  We have different languages on different evenings.  Monday Spanish, Tuesday Italian, Wednesday German and Friday French.  On Thursday, we usually have a meal in a restaurant for anyone who wants to come.  Well.  that sounds great.  I really need to practice my French.  Okay, well, if you can just give me your name and address, I'll send you the form and some more information.  If you join now, you can have .  the first .  month free试音到此结束。 听力考试正式开始。 请看听力部分，第一节第一节听下面5段对话，每段对话后有一个小题，从题中所给的ABC3个选项中选出最佳选项。 听完每段对话后，你都有10秒钟的时间来回答有关小题和阅读下一小题，每段对话仅读一遍。 例如现在你有5秒钟的时间看试卷上的例题。 你将听到以。 下内容，excuse me.  the shirt is it's 9:15.  你将有5秒钟的时间将正确答案标在试卷上。 衬衫的价格为九磅，15便是，所以你选择C项并将其标在试卷上。 现在你有5秒钟的时间阅读第一小题的有关内容。 Thanks for the wonderful weekend.  Kate.  That's okay.  Bob and I are glad you came to see us.  Oh, I have to go in my flight.  Will take off soon.  Do contact me when you're .  in Sydney.  Sure we will.  Paul, listen to the radio.  It's you've stolen my heart, one of the songs played at .  our wedding.  Yeah, how beautiful.  It has been popular for almost two decades.  David, forget about mark.  His aunt is in town, so he can't go .  with us today.  What a pity.  It's the last day of the art show.  How may I help you?  I bought .  a desk and asked for it to be delivered to my .  house this Friday.  Yes, what's the problem?  I need to have it delivered this Saturday.  Next, please.  Oh.  hi.  I missed my 9:00 train to Bedford.  Do I have to buy another ticket?  No.  The next train leaves at a 9:45 at platform eleven.  Thank you.  第一节到此结束。 第二节听下面5段对话或独白。 每段对话或独白后有几个小题，从题中所给的ABC3个选项中选出最佳选项。 听每段对话或读白前，你将有时间阅读各个小题，每小题5秒钟。 听完后，各小题将给出5秒钟的作答时间，每段对话或独白读两遍。 听下面一段对话，回答第六和第7两个小题。 现在你有10秒钟的时间阅读这两个小题。 Honey, have you checked today's weather .  forecast?  Yes, it's cold and wet.  There is a warning for strong winds up to 100 km per hour.  What are we going to do then?  Nothing much.  Just stay indoors.  There is a risk, falling trees and power lines.  right?  And the low temperatures could bring snow to the forest area.  I hope it's over quickly.  Well, it won't get better until late Wednesday anyway.  I have to move the car away .  from the trees.  Yeah, you cannot not be too careful, honey.  Have you checked today's weather .  forecast?  Yes, it's cold and wet.  There is a warning for strong winds up to 100 km per hour.  What are we going to do then?  Nothing much.  Just stay indoors.  There is a risk falling trees and power lines.  right?  And the low temperatures .  could bring snow to the forest area.  I hope it's over quickly.  Well, it won't get better until late Wednesday anyway.  I have to move the car away .  from the trees.  Yeah, you cannot not be too careful.  听下面一段对话，回答第八至第13个小题。 现在你有15秒钟的时间阅读这三个小题。 Hello Dave.  This is Kathy from Sunny California.  Hi Kathy, you finally called.  How was the move all settled in?  Sorry I .  hadn't called sooner, but it's been a busy month.  We're slowly getting things set up in our new home.  Yeah, I understand.  How are Jeff and the children?  Jeff is doing well with his new job.  Tom has made many new friends here and has a lot to do.  Fiona is fine though she misses her grandma.  By the way, thank you for looking in on my mother from time to time.  I call her every week, but it isn't the same as .  seeing her.  No problem.  Betty and I are friends now.  How is the .  weather there?  It's nice and warm and we are able to spend some time every week on the beach with the children.  That's great.  Hello Dave.  Is Kathy .  from Sunny California?  Hi Kathy.  You finally called.  How was the move all settled in?  Sorry I hadn't called sooner.  but it's been a busy month.  We're slowly getting things set up in our new home.  Yeah, I understand.  How are Jeff and the children?  Jeff is doing well with his new job.  Tom has made many new friends here and has a lot to do.  Fiona is fine, though.  She misses her grandma.  By the way, thank you for looking in on my mother from time to time.  I call her every week, but it isn't the .  same as seeing her.  No problem.  Betty and I are friends now.  How is the weather there?  It's nice and warm, and we are able to spend some time every week on the beach .  with the children.  That's great.  听下面一段对话，回答第11至第13 3个小题。 现在您有15秒钟的时间阅读这三个小题。 Jack, how did you get to school when you were in primary school?  I lived close to my school, so I walked every day.  Why?  Well, I remember .  that when we were kids, we often walked, rode a bike or caught the bus to school.  Few of us were dropped off at the school gate .  by our parents.  I see what you mean.  These days, you can see traffic jams around schools that drop off and pick kup times, but it's hard to blame the parents.  They have good reasons for driving their kids to school, mostly to do with safety .  and convenience.  You have a point there, but it could also mean children are missing out on much needed exercise and other life skills.  Some parents are just being overprotective with their children, learning nothing but living in fear of everything.  Studies have found that children who spend more time outside tend to be healthier, better adjusted and better at dealing with stress.  Jack, how did you get to school when you were in primary school?  I lived close to my school, so I walked every day.  Why?  Well.  I remember that when we were kids, we often walked, rode a bike or caught the bus to school.  Few of us were dropped off at the school gate .  by our parents.  I see what you mean.  These days, you can see traffic jams around schools that drop off and pick kup times, but it's hard to blame the parents.  They have good reasons for driving their kids to school, mostly to do with safety .  and convenience.  You have a point there, but it could also mean children are missing out on much needed exercise and other life skills.  Some parents are just being overprotective with their children, learning nothing but living in fear of everything.  Studies have found that children who spend more time outside tend to be healthier, better adjusted and better at dealing with stress.  听下面一段对话，回答第14至第17 4个小题。 现在你有20秒钟的时间阅读这四个小题。 So Marie, your kitchen garden looks excellent.  What made you turn to social media to record your vegetable growing?  Initially, I used the online platform as a diary, something to look back on, giving me a sense of achievement and keeping me motivated and moving forward.  As time went by, other gardeners and like minded people began to follow my progress too.  I know you grow lots of fruit on your land.  Which would you recommend to beginners .  as the best to grow strawberries would be a good choice.  They produce a lot of fruit in their .  first season.  That's cool.  Well, do you have plans to try new or any particular .  crops next year?  Next season I will be adding some pear trees to the fruit area.  I will be adding more herbs, which I can use in the kitchen.  And after a couple of years of failure, I will try .  growing carrots again.  What advice would you offer someone thinking of doing kitchen gardening?  Have a plan of what you want your kitchen garden to look like.  Don't be too discouraged if things don't go according to plan.  Learn from your mistakes and move on.  There's is always next season.  So Marie, your kitchen garden looks excellent.  What made you turn to social media to record your vegetable growing?  Initially, I used the online platform as a diary, something to look back on, giving me a sense of achievement and keeping me motivated and moving forward.  As time went by, other gardeners and like minded people began to follow my progress too.  I know you grow lots of fruit on your land.  Which would you recommend to beginners .  as the best to grow strawberries would be a good choice.  They produce a lot of fruit in their .  first season.  That's cool.  Well, do you have plans to try new or any particular .  crops next year?  Next season, I will be adding some pear trees to the fruit area, I will be adding more herbs, which I can use in the kitchen, and after a couple of years of failure, I will try .  growing carrots again.  What advice would you offer someone thinking of doing kitchen .  gardening?  Have a plan of what you want your kitchen garden to look like.  Don't be too discouraged if things don't go according to plan.  Learn from your mistakes and move on.  There is always next season.  拼下面一段独白，回答第18至第23个小题。 现在你有15秒钟的时间阅读这三个小题。 Welcome to meet the author.  Well, many readers of sports times turn to the last page of their magazine first in order to read Jacob Johnson's weekly article under the title life of Johnson.  The articles, along with his novels and essay collections, have earned Johnson the reputation as one of the funniest humans on the planet.  Johnson began writing about sports as a second year student at the University of\tColorado, covering high school volleyball games for his hometown newspaper.  After graduating in 1981, he moved on to work at the Denver weekly for two years and the Los Angeles post for two more years before landing at sports times.  He has been voted national sports writer of the year eleven times.  So now let's welcome the funny man with serious talent, Jacob Johnson.  Welcome to meet the author.  Well, many readers of sports times turn to the last page of their magazine first in order to read Jacob Johnson's weekly article under the title life of Johnson.  The articles, along with his novels and essay collections, have earned Johnson the reputation as one of the funniest humans on the planet.  Johnson began writing about sports as a second year student at the University of\tColorado, covering high school volleyball games for his hometown newspaper.  After graduating in 1981, he moved on to work at the Denver weekly for two years and the Los Angeles post for two more years before landing at sports times.  He has been voted national sports writer of the year eleven times.  So now let's welcome the funny man with serious talent, Jacob Johnson.  第二节到此结束，现在你有两分钟的时间将试卷上的答案转图到答题卡上。 听力部分到此结束。\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "Wlh-qzZX7wPv"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@simple(llm_client=DEFAULT_TEXT_LLM,model=\"deepseek-chat\")\n",
        "def doc_sketch(txt: str,max_chunks, chunk_size) -> str:\n",
        "    chunk_extractor = DocumentChunkExtractor()\n",
        "    txt = chunk_extractor.extract_important_chunks(doc_text=content, max_chunks=10,chunk_size=256)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return f\"\"\"\n",
        "\n",
        "      你是一位音频理解与信息结构化专家，请分析以下音频内容，并根据音频实际情况提取摘要信息，输出标准 JSON 格式，并根据类型完成以下任务：\n",
        "      0.音频的语言：用英文简写（en，zh_CN等）如有多种语言请全部给出\n",
        "      1.总结出音频的内容的标题\n",
        "      2.生成音频内容的摘要描述，按照文本的前后顺序，完整的讲述音频中的内容，字数控制在400-500之间\n",
        "      3.给出音频中提到的关键对象，按照时间顺序依次输出，选择最重要的关键对象，不要超过二十个\n",
        "      4.音频发生的场景（如有多个就依次打出）\n",
        "      5.根据文本语义确定音频中说话人的数量，关系\n",
        "\n",
        "\n",
        "      请严格按照下面的例子的格式输出 JSON 格式内容，不要添加解释说明：\n",
        "          {{\n",
        "            \"language\":\"en\",\n",
        "            \"topic\": \"寒冷天气与汽车启动问题\",\n",
        "            \"summary\": \"音频描述了两位说话人讨论寒冷天气的影响，尤其是汽车启动困难的问题。提到气温为零下18度，车主没有车库，汽车启动困难以及如何通过喝咖啡取暖。\",\n",
        "            \"objects\": [ \"汽车\", \"车库\", \"咖啡\", \"停车场\",\"天气\"],\n",
        "            \"scene\": \"场景:寒冷的室外环境，停车场\",\n",
        "            \"relationship\": \"数量:2,关系:情侣\"\n",
        "            }}\n",
        "        输入音频文本如下：{txt}\n",
        "    \"\"\"\n",
        "result = doc_sketch(txt=content,max_chunks=10,chunk_size=256)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5FpvYTTvOug",
        "outputId": "0ee5ecba-155b-4010-eb64-9e915c590e46"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"language\": \"zh_CN\",\n",
            "  \"topic\": \"英语听力考试与社交俱乐部介绍\",\n",
            "  \"summary\": \"音频内容主要包括两部分：第一部分是2024年普通高等学校招生全国统一考试英语科听力部分的试音内容，介绍了国际朋友社交俱乐部的活动，包括语言交流晚会和社交活动。第二部分是听力考试的正式内容，包含多段对话，涉及天气警告、搬家后的生活情况、孩子上学方式的变化以及园艺爱好者的经验分享。最后还介绍了体育作家Jacob Johnson的职业生涯和成就。\",\n",
            "  \"objects\": [\"国际朋友社交俱乐部\", \"语言交流晚会\", \"天气警告\", \"搬家\", \"孩子上学方式\", \"园艺\", \"草莓\", \"梨树\", \"胡萝卜\", \"社交媒体\", \"体育作家\", \"Jacob Johnson\", \"体育杂志\", \"高中排球比赛\", \"丹佛周刊\", \"洛杉矶邮报\", \"体育时报\", \"国家体育作家奖\"],\n",
            "  \"scene\": \"场景:英语听力考试现场，社交俱乐部，家庭生活，学校，园艺场地，体育杂志编辑部\",\n",
            "  \"relationship\": \"数量:多段对话中的不同说话人，关系:考生与考官，俱乐部工作人员与潜在会员，朋友之间，家庭成员，园艺爱好者与采访者，主持人与嘉宾\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    }
  ]
}