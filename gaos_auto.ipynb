{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORNe1Bn88atVOpnFmlWQYr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shi991027/RAG/blob/main/gaos_auto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers  torch\n"
      ],
      "metadata": {
        "id": "1YJ4DyBFATmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVOVAJg3A9AR",
        "outputId": "576c8a99-06a8-4630-ee4e-a07be6342556",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deepseek_tokenizer"
      ],
      "metadata": {
        "id": "W-OR8R6PN6Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai\n"
      ],
      "metadata": {
        "id": "3lItS6IMUaAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from typing import List, Dict, Union, Any\n",
        "import logging\n",
        "\n",
        "# 配置日志\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger('bge_m3_client')\n",
        "\n",
        "\n",
        "class BGEM3Client:\n",
        "    \"\"\"\n",
        "    BGE-M3 嵌入模型客户端\n",
        "\n",
        "    这个客户端封装了对 Silicon Flow API 的调用，用于获取文本嵌入向量。\n",
        "    支持单条和批量文本处理，自动处理批量限制。\n",
        "    \"\"\"\n",
        "\n",
        "    # API 配置\n",
        "    DEFAULT_API_URL = \"https://api.siliconflow.cn/v1/embeddings\"\n",
        "    DEFAULT_MODEL = \"Pro/BAAI/bge-m3\"\n",
        "    MAX_BATCH_SIZE = 64  # 根据测试结果确定的最大批量大小\n",
        "    MAX_TOKEN_LENGTH = 8192  # 单个文本最大Token数量\n",
        "    APPROX_CHAR_PER_TOKEN = 1.2  # 每个Token大约对应的字符数(粗略估计)\n",
        "    MAX_CHAR_LENGTH = int(MAX_TOKEN_LENGTH / APPROX_CHAR_PER_TOKEN)  # 大约为10000字符\n",
        "    VECTOR_DIMENSION = 1024  # 嵌入向量维度\n",
        "\n",
        "    def __init__(self, api_key: str, api_url: str = DEFAULT_API_URL, model: str = DEFAULT_MODEL):\n",
        "        \"\"\"\n",
        "        初始化 BGE-M3 客户端\n",
        "\n",
        "        参数:\n",
        "            api_key: Silicon Flow API 密钥\n",
        "            api_url: API 端点URL，默认为 \"https://api.siliconflow.cn/v1/embeddings\"\n",
        "            model: 使用的模型名称，默认为 \"Pro/BAAI/bge-m3\"\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.api_url = api_url\n",
        "        self.model = model\n",
        "        self.headers = {\n",
        "            \"Authorization\": f\"Bearer {api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        logger.info(f\"已初始化 BGE-M3 客户端，模型: {model}\")\n",
        "\n",
        "    def _check_text_length(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        检查文本长度是否超过限制\n",
        "\n",
        "        参数:\n",
        "            text: 需要检查的文本\n",
        "\n",
        "        返回:\n",
        "            如果文本长度在限制范围内返回 True，否则返回 False\n",
        "        \"\"\"\n",
        "        return len(text) <= self.MAX_CHAR_LENGTH\n",
        "\n",
        "    def get_embeddings(self, texts: Union[str, List[str]],\n",
        "                       batch_size: int = MAX_BATCH_SIZE,\n",
        "                       retry_count: int = 3,\n",
        "                       retry_delay: float = 1.0,\n",
        "                       encoding_format: str = \"float\") -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        获取文本的嵌入向量\n",
        "\n",
        "        参数:\n",
        "            texts: 单条文本字符串或文本列表\n",
        "            batch_size: 批处理大小，默认为最大批量大小 (64)\n",
        "            retry_count: 重试次数\n",
        "            retry_delay: 重试延迟（秒）\n",
        "            encoding_format: 编码格式，默认为 \"float\"\n",
        "\n",
        "        返回:\n",
        "            包含嵌入向量结果的字典，格式如下:\n",
        "            {\n",
        "                \"embeddings\": 嵌入向量列表,\n",
        "                \"total_tokens\": 处理的令牌总数,\n",
        "                \"model\": 使用的模型名称,\n",
        "                \"dimensions\": 向量维度,\n",
        "                \"batch_stats\": {\n",
        "                    \"total_batches\": 总批次数,\n",
        "                    \"total_time\": 总处理时间,\n",
        "                    \"avg_time_per_batch\": 每批平均处理时间,\n",
        "                    \"avg_time_per_text\": 每文本平均处理时间\n",
        "                }\n",
        "            }\n",
        "        \"\"\"\n",
        "        # 统一输入格式\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        # 所有texts强制小写\n",
        "        texts = [text.lower() for text in texts]\n",
        "\n",
        "        # 参数验证\n",
        "        if not texts:\n",
        "            return {\"embeddings\": [], \"total_tokens\": 0, \"model\": self.model, \"dimensions\": self.VECTOR_DIMENSION}\n",
        "\n",
        "        # 检查文本长度\n",
        "        for i, text in enumerate(texts):\n",
        "            if not self._check_text_length(text):\n",
        "                logger.warning(f\"第 {i+1} 条文本长度超过限制 (字符数: {len(text)}, 限制: {self.MAX_CHAR_LENGTH})。该文本可能会被API拒绝。\")\n",
        "\n",
        "        # 确保批大小不超过最大限制\n",
        "        actual_batch_size = min(batch_size, self.MAX_BATCH_SIZE)\n",
        "        if batch_size > self.MAX_BATCH_SIZE:\n",
        "            logger.warning(f\"请求的批大小 {batch_size} 超过最大限制 {self.MAX_BATCH_SIZE}，已自动调整\")\n",
        "\n",
        "        # 将文本划分为批次\n",
        "        batches = [texts[i:i + actual_batch_size] for i in range(0, len(texts), actual_batch_size)]\n",
        "        all_embeddings = []\n",
        "        total_time = 0\n",
        "        batch_times = []\n",
        "\n",
        "        logger.info(f\"开始处理 {len(texts)} 条文本，分为 {len(batches)} 个批次，每批最大 {actual_batch_size} 条\")\n",
        "\n",
        "        # 处理每个批次\n",
        "        for i, batch in enumerate(batches):\n",
        "            success = False\n",
        "            attempts = 0\n",
        "            batch_result = None\n",
        "\n",
        "            while not success and attempts < retry_count:\n",
        "                try:\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    # 构建请求\n",
        "                    payload = {\n",
        "                        \"model\": self.model,\n",
        "                        \"input\": batch,\n",
        "                        \"encoding_format\": encoding_format\n",
        "                    }\n",
        "\n",
        "                    # 发送请求\n",
        "                    response = requests.post(\n",
        "                        self.api_url,\n",
        "                        headers=self.headers,\n",
        "                        json=payload,\n",
        "                        timeout=30  # 30秒超时\n",
        "                    )\n",
        "\n",
        "                    end_time = time.time()\n",
        "                    batch_time = end_time - start_time\n",
        "                    batch_times.append(batch_time)\n",
        "                    print(batch_time)\n",
        "                    total_time += batch_time\n",
        "\n",
        "                    # 检查响应状态\n",
        "                    if response.status_code == 200:\n",
        "                        batch_result = response.json()\n",
        "\n",
        "                        # 增加详细日志，记录API返回内容\n",
        "                        content_size = len(str(batch_result))\n",
        "                        logger.info(f\"API响应内容大小: {content_size} 字符\")\n",
        "\n",
        "                        # 检查data字段\n",
        "                        if \"data\" in batch_result:\n",
        "                            data_count = len(batch_result[\"data\"])\n",
        "                            input_count = len(batch)\n",
        "                            logger.info(f\"API处理结果: 输入 {input_count} 个文本，返回 {data_count} 个向量\")\n",
        "\n",
        "                            # 如果数量不匹配，记录更详细信息\n",
        "                            if data_count < input_count:\n",
        "                                logger.warning(f\"API返回的向量数量({data_count})少于输入文本数量({input_count})\")\n",
        "\n",
        "                                # 记录部分API响应内容，避免日志过大\n",
        "                                sample_keys = list(batch_result.keys())\n",
        "                                logger.info(f\"API响应包含以下字段: {sample_keys}\")\n",
        "\n",
        "                                # 如果有错误信息字段，记录下来\n",
        "                                if \"error\" in batch_result:\n",
        "                                    logger.error(f\"API报告错误: {batch_result['error']}\")\n",
        "                                elif \"errors\" in batch_result:\n",
        "                                    logger.error(f\"API报告错误: {batch_result['errors']}\")\n",
        "                        else:\n",
        "                            logger.warning(f\"API响应中不包含'data'字段: {list(batch_result.keys())}\")\n",
        "\n",
        "                        success = True\n",
        "                        logger.info(f\"批次 {i+1}/{len(batches)} 处理成功，{len(batch)} 条文本，耗时 {batch_time:.4f} 秒\")\n",
        "                    else:\n",
        "                        # 如果状态码为429，增加重试延迟\n",
        "                        if response.status_code == 429:\n",
        "                            wait_time = retry_delay * (2 ** attempts)  # 指数增长重试延迟\n",
        "                            logger.warning(f\"批次 {i+1}/{len(batches)} 请求被限制，等待 {wait_time:.2f} 秒后重试\")\n",
        "                            time.sleep(wait_time)\n",
        "                        else:\n",
        "                            logger.warning(f\"批次 {i+1}/{len(batches)} 请求失败，状态码: {response.status_code}，响应: {response.text}，尝试 {attempts+1}/{retry_count}\")\n",
        "                            attempts += 1\n",
        "                            time.sleep(retry_delay)\n",
        "                except requests.exceptions.Timeout:\n",
        "                    wait_time = retry_delay * (1.5 ** attempts)  # 指数增长重试延迟，避免429状态码\n",
        "                    logger.warning(f\"批次 {i+1}/{len(batches)} 请求超时，等待 {wait_time:.2f} 秒后重试\")\n",
        "                    time.sleep(wait_time)\n",
        "                    attempts += 1\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"批次 {i+1}/{len(batches)} 发生异常: {str(e)}，尝试 {attempts+1}/{retry_count}\")\n",
        "                    attempts += 1\n",
        "                    time.sleep(retry_delay)\n",
        "\n",
        "            if not success:\n",
        "                logger.error(f\"批次 {i+1}/{len(batches)} 在 {retry_count} 次尝试后失败\")\n",
        "                raise Exception(f\"无法处理批次 {i+1}/{len(batches)}，所有重试都失败\")\n",
        "\n",
        "            # 提取嵌入向量\n",
        "            if \"data\" in batch_result:\n",
        "                batch_embeddings = [item[\"embedding\"] for item in batch_result[\"data\"]]\n",
        "                all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "        # 计算统计信息\n",
        "        avg_time_per_batch = total_time / len(batches) if batches else 0\n",
        "        avg_time_per_text = total_time / len(texts) if texts else 0\n",
        "\n",
        "        # 构建结果\n",
        "        result = {\n",
        "            \"embeddings\": all_embeddings,\n",
        "            \"total_tokens\": len(texts),  # 简化的令牌计数（实际应该来自API响应）\n",
        "            \"model\": self.model,\n",
        "            \"dimensions\": self.VECTOR_DIMENSION,\n",
        "            \"batch_stats\": {\n",
        "                \"total_batches\": len(batches),\n",
        "                \"total_time\": total_time,\n",
        "                \"avg_time_per_batch\": avg_time_per_batch,\n",
        "                \"avg_time_per_text\": avg_time_per_text\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def embed_query(self, query: str):\n",
        "        \"\"\"\n",
        "        将查询文本转换为嵌入向量\n",
        "\n",
        "        参数:\n",
        "            query: 需要转换的查询文本\n",
        "\n",
        "        返回:\n",
        "            嵌入向量数组\n",
        "        \"\"\"\n",
        "        # 检查查询文本是否为空\n",
        "        if not query or not query.strip():\n",
        "            error_msg = \"查询文本不能为空\"\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "        try:\n",
        "            # 获取嵌入向量\n",
        "            embedding_result = self.get_embeddings(query)\n",
        "\n",
        "            if \"embeddings\" in embedding_result and len(embedding_result[\"embeddings\"]) > 0:\n",
        "                # 返回嵌入向量\n",
        "                return embedding_result[\"embeddings\"][0]\n",
        "            else:\n",
        "                # 错误处理：无法获取嵌入向量\n",
        "                error_msg = f\"无法获取嵌入向量，API返回结果: {embedding_result}\"\n",
        "                logger.error(error_msg)\n",
        "                raise ValueError(error_msg)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"嵌入模型处理异常: {str(e)}\"\n",
        "            logger.error(error_msg)\n",
        "            # 直接抛出异常\n",
        "            raise RuntimeError(error_msg) from e\n",
        "def read_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(file_path, 'r', encoding='gbk') as file:\n",
        "            return file.read()\n",
        "def split_text(text, chunk_size=512):\n",
        "    \"\"\"\n",
        "    将文本按固定字符数进行分块\n",
        "\n",
        "    参数:\n",
        "        text: 输入的完整文本字符串\n",
        "        chunk_size: 每个块的最大字符数\n",
        "\n",
        "    返回:\n",
        "        分块后的文本列表，每个元素为一个字符串\n",
        "    \"\"\"\n",
        "    chunks = [text[i:i+chunk_size].strip() for i in range(0, len(text), chunk_size)]\n",
        "    return chunks\n",
        "# 示例用法\n",
        "def example_usage():\n",
        "    \"\"\"\n",
        "    BGE-M3 客户端示例用法\n",
        "\n",
        "    \"\"\"\n",
        "    file_path = \"2.txt\"\n",
        "    text = read_file(file_path)\n",
        "    chunks = split_text(text,30)\n",
        "    # API 密钥\n",
        "    API_KEY = \"sk-jpkuroorxergfxzgdwtznqppggwualbfruicevnhtgukrxrz\"\n",
        "\n",
        "    # 初始化客户端\n",
        "    client = BGEM3Client(api_key=API_KEY)\n",
        "\n",
        "    # 单个文本示例\n",
        "    # single_text = \"深度学习和人工智能技术正在改变世界\"\n",
        "    # result = client.get_embeddings(single_text)\n",
        "    # print(f\"单个文本嵌入维度: {len(result['embeddings'][0])}\")\n",
        "\n",
        "    # 批量文本示例\n",
        "    texts = [\n",
        "        \"政府工作报告 ——2025年3月5日在第十四届全国人民代表大会第世界\",\n",
        "        \"自然语言处理是人工智能的重要分支\",\n",
        "        \"向量数据库可以高效存储和检索嵌入向量\",\n",
        "        \"大型语言模型具有强大的文本生成能力\",\n",
        "        \"机器学习算法可以从数据中学习模式\"\n",
        "    ]\n",
        "\n",
        "    batch_result = client.get_embeddings(texts)\n",
        "    print(batch_result)\n",
        "    print(f\"批量处理结果:\")\n",
        "    print(f\"嵌入向量数量: {len(batch_result['embeddings'])}\")\n",
        "    print(f\"处理时间: {batch_result['batch_stats']['total_time']:.4f} 秒\")\n",
        "    print(f\"每文本平均时间: {batch_result['batch_stats']['avg_time_per_text']:.4f} 秒\")\n",
        "\n",
        "    # 查询文本示例\n",
        "    query_text = \"人工智能技术\"\n",
        "    query_embedding = client.embed_query(query_text)\n",
        "    print(f\"查询文本嵌入向量: {query_embedding}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    example_usage()"
      ],
      "metadata": {
        "id": "Bs3x3xBeOa8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47xgkhMzAKLx",
        "outputId": "11b42149-f7de-4278-8aee-9f094738e8f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings or chunks not found. Generating embeddings and performing clustering...\n",
            "分块数量: 39\n",
            "508\n",
            "451\n",
            "476\n",
            "489\n",
            "480\n",
            "500\n",
            "490\n",
            "484\n",
            "491\n",
            "489\n",
            "468\n",
            "448\n",
            "459\n",
            "455\n",
            "471\n",
            "458\n",
            "497\n",
            "510\n",
            "495\n",
            "484\n",
            "480\n",
            "475\n",
            "468\n",
            "456\n",
            "462\n",
            "429\n",
            "447\n",
            "481\n",
            "468\n",
            "485\n",
            "460\n",
            "480\n",
            "488\n",
            "518\n",
            "487\n",
            "484\n",
            "481\n",
            "463\n",
            "299\n",
            "5.210302829742432\n",
            "Text embeddings saved to txt_output/embeddings.npy\n",
            "Text chunks saved to txt_output/chunks.pkl\n",
            "Clustering results saved to txt_output/clustering_results.csv\n",
            "Cluster centers saved to txt_output/cluster_centers.pkl\n",
            "Closest chunks saved to txt_output/closest_chunks.csv\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from deepseek_tokenizer import ds_token\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib  # 用于保存模型和聚类中心\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "# 读取文件\n",
        "def read_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            return file.read()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(file_path, 'r', encoding='gbk') as file:\n",
        "            return file.read()\n",
        "\n",
        "\n",
        "def split_text(text, chunk_size=256):\n",
        "    \"\"\"\n",
        "    将文本转换为 tokens，按 token 数量进行切分，然后将每个 token 切分块重新转换为汉字返回。\n",
        "\n",
        "    参数:\n",
        "        text: 输入的完整文本字符串\n",
        "        chunk_size: 每个块的最大 token 数量\n",
        "\n",
        "    返回:\n",
        "        分块后的文本列表，每个元素为一个字符串\n",
        "    \"\"\"\n",
        "    # 将文本编码为 tokens\n",
        "    token_list = ds_token.encode(text)\n",
        "\n",
        "    # 按照指定的 chunk_size 将 token 切分\n",
        "    chunks_tokens = [token_list[i:i + chunk_size] for i in range(0, len(token_list), chunk_size)]\n",
        "\n",
        "    # 将每个 token 切分块重新解码为汉字并返回\n",
        "    chunks = [ds_token.decode(tokens) for tokens in chunks_tokens]\n",
        "    print(\"分块数量:\", len(chunks))\n",
        "    for i, chunk in enumerate(chunks):\n",
        "      print(len(chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# 用于确定最优聚类数的 MeanShift（此处暂不启用）\n",
        "from sklearn.cluster import MeanShift\n",
        "\n",
        "\n",
        "def determine_optimal_clusters(embeddings, max_bandwidth=10, num_steps=10):\n",
        "    cluster_counts = []  # 用于存储每个bandwidth对应的聚类数\n",
        "\n",
        "    bandwidths = np.linspace(0.1, max_bandwidth, num_steps)\n",
        "    for bandwidth in bandwidths:\n",
        "        mean_shift = MeanShift(bandwidth=bandwidth)\n",
        "        mean_shift.fit(embeddings)\n",
        "        cluster_count = len(np.unique(mean_shift.labels_))\n",
        "        cluster_counts.append(cluster_count)\n",
        "\n",
        "    plt.plot(bandwidths, cluster_counts, marker='o')\n",
        "    plt.xlabel('Bandwidth')\n",
        "    plt.ylabel('Number of Clusters')\n",
        "    plt.title('Mean Shift Clustering for Optimal Bandwidth')\n",
        "    plt.savefig('txt_output/output.png')\n",
        "\n",
        "    optimal_bandwidth_index = np.argmin(np.diff(cluster_counts)) + 1\n",
        "    optimal_bandwidth = bandwidths[optimal_bandwidth_index]\n",
        "    optimal_cluster_count = cluster_counts[optimal_bandwidth_index]\n",
        "\n",
        "    return optimal_cluster_count\n",
        "\n",
        "\n",
        "# 获取嵌入表示\n",
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embedding = outputs.last_hidden_state.mean(dim=1)  # 取平均池化作为文档的嵌入\n",
        "    return embedding.squeeze().numpy()\n",
        "\n",
        "\n",
        "def kmeans_clustering(embeddings, chunks, n_clusters=3, random_state=0):\n",
        "    # 使用 KMeans 聚类算法\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(embeddings)\n",
        "    labels = kmeans.predict(embeddings)\n",
        "    cluster_centers = kmeans.cluster_centers_\n",
        "\n",
        "    # 为每个聚类中心找到最接近的文本块\n",
        "    closest_chunks = []\n",
        "    for cluster_idx in range(n_clusters):\n",
        "        # 获取当前聚类中心\n",
        "        cluster_center = cluster_centers[cluster_idx]\n",
        "        # 找到当前聚类中所有文本的索引\n",
        "        cluster_indices = [i for i, label in enumerate(labels) if label == cluster_idx]\n",
        "\n",
        "        best_chunk_idx = None\n",
        "        best_similarity = -1  # 最小的相似度\n",
        "        # 计算所有文本块与当前聚类中心的相似度，并选择最接近的文本块\n",
        "        for idx in cluster_indices:\n",
        "            chunk_embedding = embeddings[idx]\n",
        "            similarity = cosine_similarity([chunk_embedding], [cluster_center])[0][0]\n",
        "            if similarity > best_similarity:\n",
        "                best_similarity = similarity\n",
        "                best_chunk_idx = idx\n",
        "\n",
        "        # 保存最接近的文本块以及该聚类的标签和相似度\n",
        "        closest_chunks.append({\n",
        "            \"Cluster Label\": cluster_idx,\n",
        "            \"Closest Chunk\": chunks[best_chunk_idx],\n",
        "            \"Similarity\": best_similarity\n",
        "        })\n",
        "\n",
        "    return labels, n_clusters, cluster_centers, closest_chunks\n",
        "\n",
        "\n",
        "# 使用高斯混合模型（GMM）进行聚类\n",
        "def gmm_clustering(embeddings, n_clusters=3, random_state=0):\n",
        "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state).fit(embeddings)\n",
        "    labels = gm.predict(embeddings)\n",
        "    return labels, n_clusters, gm.means_  # 返回聚类标签和聚类中心\n",
        "\n",
        "\n",
        "# 保存聚类结果、聚类中心和文本片段\n",
        "def save_clustering_results(chunks, labels, cluster_centers, closest_chunks,\n",
        "                            file_name=\"txt_output/clustering_results.csv\",\n",
        "                            cluster_centers_file=\"txt_output/cluster_centers.pkl\",\n",
        "                            closest_chunks_file=\"txt_output/closest_chunks.csv\"):\n",
        "    # 保存聚类结果\n",
        "    df = pd.DataFrame({\n",
        "        'Text Chunk': chunks,\n",
        "        'Cluster Label': labels\n",
        "    })\n",
        "    df.to_csv(file_name, index=False, encoding='utf-8-sig')\n",
        "    print(f\"Clustering results saved to {file_name}\")\n",
        "\n",
        "    # 保存聚类中心\n",
        "    joblib.dump(cluster_centers, cluster_centers_file)\n",
        "    print(f\"Cluster centers saved to {cluster_centers_file}\")\n",
        "\n",
        "    # 保存距离每个聚类中心最近的文本块及其相似度\n",
        "    closest_chunks_df = pd.DataFrame(closest_chunks)\n",
        "    closest_chunks_df.to_csv(closest_chunks_file, index=False, encoding='utf-8-sig')\n",
        "    print(f\"Closest chunks saved to {closest_chunks_file}\")\n",
        "\n",
        "\n",
        "# 保存并加载聚类中心（保持原有功能不变）\n",
        "def load_cluster_centers(cluster_centers_file=\"cluster_centers.pkl\"):\n",
        "    cluster_centers = joblib.load(cluster_centers_file)\n",
        "    return cluster_centers\n",
        "\n",
        "\n",
        "def load_clustering_results(file_name=\"clustering_results.csv\"):\n",
        "    df = pd.read_csv(file_name, encoding='utf-8-sig')\n",
        "    labels = df['Cluster Label'].values\n",
        "    chunks = df['Text Chunk'].values\n",
        "    return labels, chunks\n",
        "\n",
        "\n",
        "# 查找最相关的聚类并返回该聚类的所有片段\n",
        "def find_most_relevant_cluster(new_text, chunks, cluster_centers, labels):\n",
        "    new_embedding = get_embedding(new_text)\n",
        "    similarities = cosine_similarity([new_embedding], cluster_centers)\n",
        "    most_relevant_cluster = np.argmax(similarities)\n",
        "    relevant_chunks = [chunks[i] for i in range(len(chunks)) if labels[i] == most_relevant_cluster]\n",
        "    return most_relevant_cluster, relevant_chunks, similarities\n",
        "\n",
        "\n",
        "#########################\n",
        "# 第一部分：文本嵌入并保存\n",
        "#########################\n",
        "def embed_and_save_text(file_path, chunk_size=512,\n",
        "                        embedding_save_path=\"txt_output/embeddings.npy\",\n",
        "                        chunks_save_path=\"txt_output/chunks.pkl\"):\n",
        "    # 读取文件并分块\n",
        "    text = read_file(file_path)\n",
        "\n",
        "    chunks = split_text(text, 256)\n",
        "    # API 密钥\n",
        "    API_KEY = \"sk-jpkuroorxergfxzgdwtznqppggwualbfruicevnhtgukrxrz\"\n",
        "\n",
        "    # 初始化客户端\n",
        "    client = BGEM3Client(api_key=API_KEY)\n",
        "    batch_result = client.get_embeddings(chunks)\n",
        "    # 获取每个文本块的嵌入\n",
        "    embeddings = batch_result['embeddings']\n",
        "    # 保存嵌入结果和文本块\n",
        "    np.save(embedding_save_path, np.array(embeddings))\n",
        "    joblib.dump(chunks, chunks_save_path)\n",
        "    print(f\"Text embeddings saved to {embedding_save_path}\")\n",
        "    print(f\"Text chunks saved to {chunks_save_path}\")\n",
        "\n",
        "    return embeddings, chunks\n",
        "\n",
        "\n",
        "#########################\n",
        "# 第二部分：从文件加载嵌入并进行聚类\n",
        "#########################\n",
        "def load_embeddings_and_chunks(embedding_save_path=\"txt_output/embeddings.npy\",\n",
        "                               chunks_save_path=\"txt_output/chunks.pkl\"):\n",
        "    embeddings = np.load(embedding_save_path, allow_pickle=True)\n",
        "    chunks = joblib.load(chunks_save_path)\n",
        "    return embeddings, chunks\n",
        "\n",
        "\n",
        "def cluster_embeddings_from_file(embedding_save_path=\"txt_output/embeddings.npy\",\n",
        "                                 chunks_save_path=\"txt_output/chunks.pkl\",\n",
        "                                 n_clusters=3):\n",
        "    # 从文件加载嵌入和文本块\n",
        "    embeddings, chunks = load_embeddings_and_chunks(embedding_save_path, chunks_save_path)\n",
        "\n",
        "    # 使用 KMeans 进行聚类，并返回聚类标签、聚类中心和最接近的文本块\n",
        "    labels, n_clusters, cluster_centers, closest_chunks = kmeans_clustering(embeddings, chunks, n_clusters)\n",
        "\n",
        "    # 保存聚类结果、聚类中心和最接近的文本块\n",
        "    save_clustering_results(chunks, labels, cluster_centers, closest_chunks)\n",
        "\n",
        "    return labels, cluster_centers, closest_chunks\n",
        "\n",
        "\n",
        "#########################\n",
        "# 修改后的主流程：先执行第一部分，再执行第二部分\n",
        "#########################\n",
        "def main(file_path, chunk_size=100,\n",
        "         embedding_save_path=\"txt_output/embeddings.npy\",\n",
        "         chunks_save_path=\"txt_output/chunks.pkl\"):\n",
        "    # 判断是否存在已保存的嵌入和文本块文件\n",
        "    if os.path.exists(embedding_save_path) and os.path.exists(chunks_save_path):\n",
        "        print(\"Found existing embeddings and chunks. Loading and clustering...\")\n",
        "        # 从文件中加载嵌入，并进行聚类\n",
        "        start_time = time.time()\n",
        "        labels, cluster_centers, closest_chunks = cluster_embeddings_from_file(embedding_save_path, chunks_save_path,\n",
        "                                                                               n_clusters=10)\n",
        "        end_time = time.time()\n",
        "        print(f\"Clustering executed in {end_time - start_time:.2f} seconds\")\n",
        "    else:\n",
        "        print(\"Embeddings or chunks not found. Generating embeddings and performing clustering...\")\n",
        "        # 第一部分：生成嵌入并保存\n",
        "        embed_and_save_text(file_path, chunk_size, embedding_save_path, chunks_save_path)\n",
        "        # 第二部分：从文件中加载嵌入，并进行聚类\n",
        "        labels, cluster_centers, closest_chunks = cluster_embeddings_from_file(embedding_save_path, chunks_save_path,\n",
        "                                                                               n_clusters=10)\n",
        "\n",
        "    return labels, cluster_centers, closest_chunks\n",
        "\n",
        "\n",
        "file_path = \"2.txt\"\n",
        "labels, cluster_centers, closest_chunks = main(file_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "gphdaBq3-FxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# 初始化 DeepSeek API 客户端\n",
        "client = OpenAI(api_key=\"sk-c553c5fa02e64d729f91e8593f914776\", base_url=\"https://api.deepseek.com\")\n",
        "# 假设我们有一个新的问题：\n",
        "question= \"\"\n",
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def concat_clusters(closest_chunks_file=\"txt_output/closest_chunks.csv\",file_path = \"2.txt\"):\n",
        "    # 读取 closest_chunks.csv 文件\n",
        "    text = read_file(file_path)\n",
        "    df = pd.read_csv(closest_chunks_file, encoding='utf-8-sig')\n",
        "    chunks = split_text(text, 256)\n",
        "    final_text = \" -- -- \".join(chunks[:2])\n",
        "\n",
        "    # 创建一个列表来存储每个聚类的最接近的文本块\n",
        "    cluster_texts = []\n",
        "\n",
        "    # 遍历每个聚类\n",
        "    for cluster_label in df['Cluster Label'].unique():\n",
        "        # 获取该聚类的第一个 \"Closest Chunk\"\n",
        "        cluster_chunk = df[df['Cluster Label'] == cluster_label].iloc[0]['Closest Chunk']\n",
        "        cluster_texts.append(cluster_chunk)\n",
        "\n",
        "    final_text += \" --||-- \".join(cluster_texts)\n",
        "\n",
        "    # 拼接剩下的 chunks\n",
        "    final_text += \" --||-- \".join(chunks[len(chunks)-2:])  # 这里将最后两个 chunks 拼接到 final_text\n",
        "    return final_text\n",
        "\n",
        "\n",
        "# 调用函数\n",
        "final_text = concat_clusters(\"txt_output/closest_chunks.csv\",\"2.txt\")\n",
        "\n",
        "\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"deepseek-chat\",  # 使用 DeepSeek 的聊天模型\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI assistant who provides accurate and concise answers based strictly on the given text. You should not make assumptions or provide external information outside of the provided content.\"},  # 系统消息，用于设置对话环境\n",
        "        {\"role\": \"user\", \"content\": f\"请根据以下提供的文档，生成一个合适的标题和一段 200-300 字的摘要。文档中的片段以 -- -- 划分。标题应简洁明了，能够准确概括文档的核心内容。摘要应完整呈现文档的主要论点、关键信息和结论，要确保一定提到文档中的每个片段，适用于后续检索和快速理解原文内容。请按照 JSON 格式输出，确保格式规范且可解析，格式如下：title:生成的标题,summary:生成的摘要，请确保 title 反映文档核心内容，summary 精炼但包括所有段落的内容。输入内容如下：{final_text}现在，不要给出任何解释性文本，请直接输出:\"}  # 用户输入，替换占位符\n",
        "    ],\n",
        "    stream=False  # 设置为 False 获取完整响应（而不是流式响应）\n",
        ")\n",
        "\n",
        "# 输出模型返回的消息内容\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLPNBWP2U0Om",
        "outputId": "ae5b7ea5-bfb7-41a6-8801-36cec35cd620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "分块数量: 39\n",
            "508\n",
            "451\n",
            "476\n",
            "489\n",
            "480\n",
            "500\n",
            "490\n",
            "484\n",
            "491\n",
            "489\n",
            "468\n",
            "448\n",
            "459\n",
            "455\n",
            "471\n",
            "458\n",
            "497\n",
            "510\n",
            "495\n",
            "484\n",
            "480\n",
            "475\n",
            "468\n",
            "456\n",
            "462\n",
            "429\n",
            "447\n",
            "481\n",
            "468\n",
            "485\n",
            "460\n",
            "480\n",
            "488\n",
            "518\n",
            "487\n",
            "484\n",
            "481\n",
            "463\n",
            "299\n",
            "```json\n",
            "{\n",
            "  \"title\": \"2025年政府工作报告：回顾2024年成就与展望2025年发展目标\",\n",
            "  \"summary\": \"2025年政府工作报告由国务院总理李强在第十四届全国人民代表大会第三次会议上提交。报告回顾了2024年我国在复杂严峻形势下取得的显著成就，包括经济稳步增长（GDP达134.9万亿元，增长5%）、就业物价稳定（新增就业1256万人）、产业升级（粮食产量1.4万亿斤，新能源汽车产量1300万辆）、创新能力提升（嫦娥六号月球采样）及生态环境改善（PM2.5浓度下降2.7%）。报告同时指出2025年重点工作：深化改革开放（推进财税金融改革、扩大制度型开放）、强化科技创新（发展新质生产力）、保障民生（提高医保补助、促进就业）、推动绿色低碳经济（碳达峰碳中和试点）及维护国家安全。报告强调坚持党的全面领导，统筹高质量发展，确保“十四五”规划圆满收官，全面推进中国式现代化。\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    }
  ]
}